{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XWs-3PSSomMG",
        "outputId": "1b7d780a-7ac6-4085-e9a6-bd057559d984"
      },
      "outputs": [],
      "source": [
        "# 必要なライブラリをインストール\n",
        "!pip uninstall mecab-python3 fugashi\n",
        "!pip install mecab-python3==1.0.5 fugashi[unidic-lite]\n",
        "!pip install transformers\n",
        "!pip install jaconv\n",
        "!pip install ginza\n",
        "!pip install spacy\n",
        "!python -m spacy download ja_ginza\n",
        "!pip install sentence-transformers\n",
        "!python -m spacy download ja_core_news_sm\n",
        "!pip install janome\n",
        "!pip install mojimoji\n",
        "!pip install openai fugashi unidic-lite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPBXdYdi_HyB"
      },
      "source": [
        "##日本語でやってみた（未完成）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDzdv_pesngM"
      },
      "source": [
        "###NLTKデータのダウンロード"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDBSUQqVsTIq",
        "outputId": "80d70b0c-d21c-4231-e94f-f3c31b776f07"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "# NLTKデータをダウンロード\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zenJsUFPsr7w"
      },
      "source": [
        "###日本語BERTモデルのセットアップ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hk9nd7AxoxgR",
        "outputId": "7e4a1bf5-ef92-4080-b239-bfd9b05228a3"
      },
      "outputs": [],
      "source": [
        "# 必要なモジュールをインポート\n",
        "from transformers import pipeline\n",
        "\n",
        "# 日本語BERTモデルのセットアップ\n",
        "def setup_model():\n",
        "    return pipeline(\"fill-mask\", model=\"cl-tohoku/bert-base-japanese\")\n",
        "\n",
        "model = setup_model()\n",
        "print(\"モデルのセットアップが完了しました！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi5ttDBMswwj"
      },
      "source": [
        "###Fugashiを用いた日本語のトークン化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xrq0wx6sbfR"
      },
      "outputs": [],
      "source": [
        "from fugashi import Tagger\n",
        "\n",
        "# 日本語トークナイザーをセットアップ\n",
        "tagger = Tagger()\n",
        "\n",
        "def tokenize_japanese(sentence):\n",
        "    return [word.surface for word in tagger(sentence)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i12ogSq1Ed8"
      },
      "source": [
        "###ひらがなに変換する処理の追加"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCedPLOV1LHM"
      },
      "outputs": [],
      "source": [
        "import jaconv\n",
        "\n",
        "def convert_to_hiragana(sentence):\n",
        "    # 漢字やカタカナをひらがなに変換\n",
        "    return jaconv.hira2kata(jaconv.kata2hira(sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yotac7Wms3sM"
      },
      "source": [
        "###リポグラム化処理の関数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gSXgXtP4fMf",
        "outputId": "14d2c5d1-2016-4765-ec04-249e27ee486c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import re\n",
        "\n",
        "# BERTモデルとトークナイザーの準備\n",
        "tokenizer = BertTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
        "model = BertForMaskedLM.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
        "\n",
        "# 禁止された文字を含む単語を置き換える関数\n",
        "def lipogram_sentence_dev(sentence, model, forbidden_chars):\n",
        "    \"\"\"\n",
        "    特定の文字を含む単語を特定の文字を含まない単語に置き換える\n",
        "    :param sentence: 入力文\n",
        "    :param model: BERTモデル\n",
        "    :param forbidden_chars: 含んではいけない文字のリスト\n",
        "    :return: リポグラム化された文\n",
        "    \"\"\"\n",
        "    # 文をトークン化\n",
        "    words = sentence.split()  # 簡易的な分割、必要に応じて形態素解析を使う\n",
        "\n",
        "    replaced_sentence = []\n",
        "\n",
        "    for word in words:\n",
        "        # 禁止された文字を含む単語をチェック\n",
        "        if any(char in word for char in forbidden_chars):\n",
        "            # 単語をマスク\n",
        "            masked_sentence = sentence.replace(word, \"[MASK]\")\n",
        "\n",
        "            # トークン化\n",
        "            inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "            # 予測されたトークンの確率を取得\n",
        "            predictions = outputs.logits[0, inputs['input_ids'][0] == tokenizer.mask_token_id]\n",
        "            predicted_token_id = torch.argmax(predictions).item()\n",
        "\n",
        "            # 予測されたトークンを取得\n",
        "            predicted_token = tokenizer.decode([predicted_token_id])\n",
        "\n",
        "            # 禁止された文字を含まないトークンを選択\n",
        "            if not any(char in predicted_token for char in forbidden_chars):\n",
        "                replaced_sentence.append(predicted_token)\n",
        "            else:\n",
        "                replaced_sentence.append(word)  # 適切な候補がなければ元の単語を使う\n",
        "        else:\n",
        "            # 禁止された文字を含まない単語はそのまま使用\n",
        "            replaced_sentence.append(word)\n",
        "\n",
        "    return \" \".join(replaced_sentence)\n",
        "\n",
        "# 実行例\n",
        "sentence = \"わたしはきのうのごご、がっこうでともだちとたのしいじかんをすごしました。\"\n",
        "forbidden_chars = ['か', 'つ']  # 置き換え対象の語彙\n",
        "\n",
        "result = lipogram_sentence_dev(sentence, model, forbidden_chars)\n",
        "print(\"リポグラム化後:\", result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbVsMnI18mwW",
        "outputId": "87c8b348-748c-422f-a304-42fa9238b5fa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import spacy\n",
        "\n",
        "# SpaCy日本語モデルのロード\n",
        "nlp = spacy.load(\"ja_core_news_sm\")\n",
        "\n",
        "# BERTモデルとトークナイザーの準備\n",
        "tokenizer = BertTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
        "model = BertForMaskedLM.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
        "\n",
        "# 禁止された文字を含む単語を置き換える関数\n",
        "def lipogram_sentence_dev(sentence, model, tokenizer, forbidden_chars):\n",
        "    \"\"\"\n",
        "    特定の文字を含む単語を特定の文字を含まない単語に置き換える\n",
        "    :param sentence: 入力文\n",
        "    :param model: BERTモデル\n",
        "    :param tokenizer: BERTトークナイザー\n",
        "    :param forbidden_chars: 含んではいけない文字のリスト\n",
        "    :return: リポグラム化された文\n",
        "    \"\"\"\n",
        "    # 形態素解析で単語に分割\n",
        "    doc = nlp(sentence)\n",
        "    words = [token.text for token in doc]\n",
        "\n",
        "    replaced_sentence = []\n",
        "\n",
        "    for word in words:\n",
        "        # 禁止された文字を含む単語をチェック\n",
        "        if any(char in word for char in forbidden_chars):\n",
        "            # 単語をマスク\n",
        "            masked_sentence = sentence.replace(word, \"[MASK]\", 1)\n",
        "\n",
        "            # トークン化\n",
        "            inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "            # 予測されたトークンの確率を取得\n",
        "            mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
        "            predictions = outputs.logits[0, mask_token_index]\n",
        "\n",
        "            # 上位候補を取得\n",
        "            top_k = 10  # 上位10候補を取得\n",
        "            top_k_ids = torch.topk(predictions, k=top_k, dim=-1).indices[0].tolist()\n",
        "            candidates = [tokenizer.decode([idx]).strip() for idx in top_k_ids]\n",
        "\n",
        "            # 禁止された文字を含まない単語を選択\n",
        "            replacement = next((candidate for candidate in candidates if not any(char in candidate for char in forbidden_chars)), word)\n",
        "\n",
        "            replaced_sentence.append(replacement)\n",
        "        else:\n",
        "            # 禁止された文字を含まない単語はそのまま使用\n",
        "            replaced_sentence.append(word)\n",
        "\n",
        "    return \"\".join(replaced_sentence)  # 日本語は単語間にスペースを挿入しない\n",
        "\n",
        "# 実行例\n",
        "sentence = \"わたしはきのうのごご、がっこうでともだちとたのしいじかんをすごしました。\"\n",
        "# sentence = \"私は昨日の午後、学校で友達と楽しい時間を過ごしました。\"\n",
        "forbidden_chars = ['か', 'つ', 'み', 'の']  # 置き換え対象の語彙\n",
        "\n",
        "result = lipogram_sentence_dev(sentence, model, tokenizer, forbidden_chars)\n",
        "print(\"リポグラム化後:\", result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "MGAtV5SbDNSj",
        "outputId": "da81191c-a77b-4e9e-eab0-6de7fe0bca14"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
        "from janome.tokenizer import Tokenizer\n",
        "\n",
        "# BERTモデルとトークナイザーの準備\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
        "model = BertForMaskedLM.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
        "\n",
        "# Janomeを使って単語ごとに分割する関数\n",
        "def split_into_words(sentence):\n",
        "    tokenizer = Tokenizer()\n",
        "    return [token.surface for token in tokenizer.tokenize(sentence)]\n",
        "\n",
        "# リポグラムのルールに従って後処理\n",
        "def apply_lipogram_rules(word):\n",
        "    # 小文字を大文字に、濁音・半濁音を清音に変換\n",
        "    mapping = {\n",
        "        'が': 'か', 'ぎ': 'き', 'ぐ': 'く', 'げ': 'け', 'ご': 'こ',\n",
        "        'ざ': 'さ', 'じ': 'し', 'ず': 'す', 'ぜ': 'せ', 'ぞ': 'そ',\n",
        "        'だ': 'た', 'ぢ': 'ち', 'づ': 'つ', 'で': 'て', 'ど': 'と',\n",
        "        'ば': 'は', 'び': 'ひ', 'ぶ': 'ふ', 'べ': 'へ', 'ぼ': 'ほ',\n",
        "        'ぱ': 'は', 'ぴ': 'ひ', 'ぷ': 'ふ', 'ぺ': 'へ', 'ぽ': 'ほ',\n",
        "        'ぃ': 'い', 'ぇ': 'え', 'ぅ': 'う', 'ぉ': 'お', 'っ': 'つ', 'ー': 'う'\n",
        "    }\n",
        "    for old, new in mapping.items():\n",
        "        word = word.replace(old, new)\n",
        "\n",
        "    return word\n",
        "\n",
        "def mask_forbidden_characters_words(sentence, forbidden_chars):\n",
        "    \"\"\"\n",
        "    禁止文字を含む単語を[MASK]に置き換える\n",
        "    \"\"\"\n",
        "    words = split_into_words(sentence)\n",
        "    masked_sentence = []\n",
        "\n",
        "    for word in words:\n",
        "        # 禁止文字を含む単語を[MASK]に置き換え\n",
        "        if any(char in word for char in forbidden_chars):\n",
        "            masked_sentence.append(\"[MASK]\")\n",
        "        else:\n",
        "            masked_sentence.append(word)\n",
        "\n",
        "    return \" \".join(masked_sentence)\n",
        "\n",
        "def predict_replacement(sentence, model, tokenizer, forbidden_chars):\n",
        "    \"\"\"\n",
        "    BERTモデルを用いて[MASK]を予測し、禁止文字を置き換える\n",
        "    \"\"\"\n",
        "    masked_sentence = mask_forbidden_characters_words(sentence, forbidden_chars)\n",
        "    inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
        "\n",
        "    # モデルで予測\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # [MASK]トークンの位置を取得\n",
        "    mask_token_indices = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
        "    predictions = outputs.logits[0, mask_token_indices]\n",
        "\n",
        "    # 各[MASK]トークンを禁止文字を含まない予測トークンで置き換える\n",
        "    replaced_sentence = masked_sentence\n",
        "    for i, mask_index in enumerate(mask_token_indices):\n",
        "        top_k = 10\n",
        "        predicted_token_ids = torch.topk(predictions[i], top_k).indices.tolist()\n",
        "        for token_id in predicted_token_ids:\n",
        "            predicted_token = tokenizer.decode([token_id]).strip()\n",
        "            if not any(char in predicted_token for char in forbidden_chars):\n",
        "                replaced_sentence = replaced_sentence.replace(\"[MASK]\", predicted_token, 1)\n",
        "                break\n",
        "        else:\n",
        "            replaced_sentence = replaced_sentence.replace(\"[MASK]\", \"\", 1)  # 適切な候補がない場合は空文字にする\n",
        "\n",
        "    # リポグラムルールを適用して結果を修正\n",
        "    final_sentence = \" \".join([apply_lipogram_rules(word) for word in replaced_sentence.split()])\n",
        "\n",
        "    return final_sentence\n",
        "\n",
        "# 実行例\n",
        "sentence = \"つきよのよるにいちまいのてがみをかきました\"\n",
        "forbidden_chars = ['つ', 'よ', 'い', 'て']  # 禁止文字\n",
        "\n",
        "# 実行\n",
        "result = predict_replacement(sentence, model, tokenizer, forbidden_chars)\n",
        "print(\"リポグラム化後:\", result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJU-wMvFIULV",
        "outputId": "eae41bfe-ffa7-4313-b531-a7b30b45fa2a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
        "from janome.tokenizer import Tokenizer\n",
        "\n",
        "# BERTモデルとトークナイザーの準備\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
        "model = BertForMaskedLM.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
        "\n",
        "# Janomeを使って単語ごとに分割する関数\n",
        "def split_into_words(sentence):\n",
        "    tokenizer = Tokenizer()\n",
        "    return [token.surface for token in tokenizer.tokenize(sentence)]\n",
        "\n",
        "# 禁止文字を含む単語を[MASK]に置き換える関数\n",
        "def mask_forbidden_characters_words(sentence, forbidden_chars):\n",
        "    \"\"\"\n",
        "    禁止文字を含む単語を[MASK]に置き換える\n",
        "    \"\"\"\n",
        "    words = split_into_words(sentence)\n",
        "    masked_sentence = []\n",
        "    masked_words = []  # 変換された単語を記録\n",
        "\n",
        "    for word in words:\n",
        "        # 禁止文字を含む単語を[MASK]に置き換え\n",
        "        if any(char in word for char in forbidden_chars):\n",
        "            masked_sentence.append(\"[MASK]\")\n",
        "            masked_words.append(word)  # 置き換えた単語を記録\n",
        "        else:\n",
        "            masked_sentence.append(word)\n",
        "\n",
        "    return \" \".join(masked_sentence), masked_words\n",
        "\n",
        "# BERTモデルを用いて[MASK]を予測し、変換候補を表示する関数\n",
        "def predict_replacement(sentence, model, tokenizer, forbidden_chars):\n",
        "    \"\"\"\n",
        "    BERTモデルを用いて[MASK]を予測し、禁止文字を置き換える\n",
        "    \"\"\"\n",
        "    masked_sentence, masked_words = mask_forbidden_characters_words(sentence, forbidden_chars)\n",
        "    inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
        "\n",
        "    # モデルで予測\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # [MASK]トークンの位置を取得\n",
        "    mask_token_indices = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
        "    predictions = outputs.logits[0, mask_token_indices]\n",
        "\n",
        "    # 各[MASK]トークンを予測し、変換候補を表示\n",
        "    predicted_words = []\n",
        "    for i, mask_index in enumerate(mask_token_indices):\n",
        "        top_k = 10\n",
        "        predicted_token_ids = torch.topk(predictions[i], top_k).indices.tolist()\n",
        "        candidate_tokens = [tokenizer.decode([token_id]).strip() for token_id in predicted_token_ids]\n",
        "        predicted_words.append(candidate_tokens)\n",
        "\n",
        "    return masked_sentence, masked_words, predicted_words\n",
        "\n",
        "# 実行例\n",
        "sentence = \"つきよのよるにいちまいのてがみをかきました\"\n",
        "forbidden_chars = ['つ', 'よ', 'い', 'て']  # 禁止文字\n",
        "\n",
        "# 実行\n",
        "masked_sentence, masked_words, predicted_words = predict_replacement(sentence, model, tokenizer, forbidden_chars)\n",
        "\n",
        "print(\"元の文:\", sentence)\n",
        "print(\"変換前の文:\", masked_sentence)\n",
        "print(\"変換された単語:\", masked_words)\n",
        "\n",
        "for i, word in enumerate(masked_words):\n",
        "    print(f\"[MASK]に変換された単語: {word}\")\n",
        "    print(f\"候補: {predicted_words[i]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAQD0FZwJP44",
        "outputId": "f505730b-67ea-4437-c7b9-ff24ea386a95"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
        "from janome.tokenizer import Tokenizer\n",
        "import mojimoji  # 漢字をひらがなに変換するためのライブラリ\n",
        "\n",
        "# BERTモデルとトークナイザーの準備\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
        "model = BertForMaskedLM.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
        "\n",
        "# Janomeを使って単語ごとに分割する関数\n",
        "def split_into_words(sentence):\n",
        "    tokenizer = Tokenizer()\n",
        "    return [token.surface for token in tokenizer.tokenize(sentence)]\n",
        "\n",
        "# 漢字をひらがなに変換する関数\n",
        "def kanji_to_hiragana(word):\n",
        "    # mojimojiで簡単なひらがな変換を行う（※実際にはもっと精度高い変換が必要）\n",
        "    return mojimoji.han_to_zen(word)\n",
        "\n",
        "# リポグラムのルールに従って後処理\n",
        "def apply_lipogram_rules(word):\n",
        "    # 小文字を大文字に、濁音・半濁音を清音に変換\n",
        "    mapping = {\n",
        "        'が': 'か', 'ぎ': 'き', 'ぐ': 'く', 'げ': 'け', 'ご': 'こ',\n",
        "        'ざ': 'さ', 'じ': 'し', 'ず': 'す', 'ぜ': 'せ', 'ぞ': 'そ',\n",
        "        'だ': 'た', 'ぢ': 'ち', 'づ': 'つ', 'で': 'て', 'ど': 'と',\n",
        "        'ば': 'は', 'び': 'ひ', 'ぶ': 'ふ', 'べ': 'へ', 'ぼ': 'ほ',\n",
        "        'ぱ': 'は', 'ぴ': 'ひ', 'ぷ': 'ふ', 'ぺ': 'へ', 'ぽ': 'ほ',\n",
        "        'ぃ': 'い', 'ぇ': 'え', 'ぅ': 'う', 'ぉ': 'お', 'っ': 'つ', 'ー': 'う'\n",
        "    }\n",
        "    for old, new in mapping.items():\n",
        "        word = word.replace(old, new)\n",
        "\n",
        "    return word\n",
        "\n",
        "def mask_forbidden_characters_words(sentence, forbidden_chars):\n",
        "    \"\"\"\n",
        "    禁止文字を含む単語を[MASK]に置き換える\n",
        "    \"\"\"\n",
        "    words = split_into_words(sentence)\n",
        "    masked_sentence = []\n",
        "\n",
        "    for word in words:\n",
        "        # 禁止文字を含む単語を[MASK]に置き換え\n",
        "        if any(char in word for char in forbidden_chars):\n",
        "            masked_sentence.append(\"[MASK]\")\n",
        "        else:\n",
        "            masked_sentence.append(word)\n",
        "\n",
        "    return \" \".join(masked_sentence)\n",
        "\n",
        "def predict_replacement(sentence, model, tokenizer, forbidden_chars):\n",
        "    \"\"\"\n",
        "    BERTモデルを用いて[MASK]を予測し、禁止文字を置き換える\n",
        "    \"\"\"\n",
        "    masked_sentence = mask_forbidden_characters_words(sentence, forbidden_chars)\n",
        "    inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
        "\n",
        "    # モデルで予測\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # [MASK]トークンの位置を取得\n",
        "    mask_token_indices = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
        "    predictions = outputs.logits[0, mask_token_indices]\n",
        "\n",
        "    # 各[MASK]トークンを禁止文字を含まない予測トークンで置き換える\n",
        "    replaced_sentence = masked_sentence\n",
        "    for i, mask_index in enumerate(mask_token_indices):\n",
        "        top_k = 10\n",
        "        predicted_token_ids = torch.topk(predictions[i], top_k).indices.tolist()\n",
        "        for token_id in predicted_token_ids:\n",
        "            predicted_token = tokenizer.decode([token_id]).strip()\n",
        "\n",
        "            # 漢字の場合、ひらがなに変換\n",
        "            hiragana_predicted = kanji_to_hiragana(predicted_token)\n",
        "\n",
        "            if not any(char in hiragana_predicted for char in forbidden_chars):\n",
        "                replaced_sentence = replaced_sentence.replace(\"[MASK]\", predicted_token, 1)\n",
        "                break\n",
        "        else:\n",
        "            replaced_sentence = replaced_sentence.replace(\"[MASK]\", \"\", 1)  # 適切な候補がない場合は空文字にする\n",
        "\n",
        "    # リポグラムルールを適用して結果を修正\n",
        "    final_sentence = \" \".join([apply_lipogram_rules(word) for word in replaced_sentence.split()])\n",
        "\n",
        "    return final_sentence\n",
        "\n",
        "# 実行例\n",
        "sentence = \"つきよのよるにいちまいのてがみをかきました\"\n",
        "forbidden_chars = ['つ', 'よ', 'い', 'て']  # 禁止文字\n",
        "\n",
        "# 実行\n",
        "result = predict_replacement(sentence, model, tokenizer, forbidden_chars)\n",
        "print(\"リポグラム化後:\", result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFBDgeKroyKE"
      },
      "outputs": [],
      "source": [
        "# リポグラム化処理の関数\n",
        "def lipogram_sentence(sentence, model, forbidden_chars):\n",
        "    \"\"\"\n",
        "    特定の文字を含む単語を特定の文字を含まない単語に置き換える\n",
        "    :param sentence: 入力文\n",
        "    :param model: BERTモデル\n",
        "    :param forbidden_chars: 含んではいけない文字のリスト\n",
        "    :return: リポグラム化された文\n",
        "    \"\"\"\n",
        "    words = tokenize_japanese(sentence)\n",
        "    replaced_sentence = []\n",
        "\n",
        "    for word in words:\n",
        "        # 禁止された文字を含む単語をチェック\n",
        "        if any(char in word for char in forbidden_chars):\n",
        "            # マスクされた文章を作成\n",
        "            masked_sentence = sentence.replace(word, \"[MASK]\", 1)\n",
        "\n",
        "            # トークン化してモデルに入力\n",
        "            inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
        "\n",
        "            # モデルに予測をさせる\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "            # 予測されたトークンを取得\n",
        "            mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
        "            predictions = outputs.logits[0, mask_token_index]\n",
        "\n",
        "            # 予測されたトークンを選択\n",
        "            predicted_token_id = torch.argmax(predictions, dim=-1).item()\n",
        "            predicted_token = tokenizer.decode([predicted_token_id])\n",
        "\n",
        "            # 禁止された文字を含まないトークンを選択\n",
        "            if not any(char in predicted_token for char in forbidden_chars):\n",
        "                replaced_sentence.append(predicted_token)\n",
        "            else:\n",
        "                replaced_sentence.append(word)  # 適切な候補がなければ元の単語を使う\n",
        "        else:\n",
        "            replaced_sentence.append(word)  # 禁止された文字を含まない単語はそのまま使用\n",
        "\n",
        "    return \"\".join(replaced_sentence)  # 文章を結合して返す"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "ywczbQfgo25m",
        "outputId": "1022db9d-00b3-4cbb-fe04-b0db21b73ed4"
      },
      "outputs": [],
      "source": [
        "# テスト用の文章\n",
        "# sentence = \"私は昨日の午後、学校で友達と楽しい時間を過ごしました。\"\n",
        "sentence = \"わたしはきのうのごご、がっこうでともだちとたのしいじかんをすごしました。\"\n",
        "forbidden_chars = [\"か\", \"つ\", \"み\", \"わ\", \"き\"]  # 禁止文字\n",
        "\n",
        "print(\"元の文:\", sentence)\n",
        "new_sentence = lipogram_sentence(sentence, model, forbidden_chars)\n",
        "print(\"リポグラム化後:\", new_sentence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyP0RRyd7raX"
      },
      "source": [
        "##英語でやってみた"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "o2Asom7v70UD",
        "outputId": "84f1ae66-f33e-4d73-ca97-b81037b62d9f"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import torch\n",
        "\n",
        "# Load English spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to tokenize English text using spaCy\n",
        "def tokenize_english_with_spacy(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    return [token.text for token in doc]\n",
        "\n",
        "# Function to replace words with forbidden characters using a BERT model\n",
        "def lipogram_sentence_english(sentence, model, tokenizer, forbidden_chars):\n",
        "    \"\"\"\n",
        "    Replace words with forbidden characters with alternatives that do not contain forbidden characters.\n",
        "    :param sentence: Input sentence\n",
        "    :param model: BERT model\n",
        "    :param tokenizer: BERT tokenizer\n",
        "    :param forbidden_chars: List of forbidden characters\n",
        "    :return: Lipogrammed sentence\n",
        "    \"\"\"\n",
        "    words = tokenize_english_with_spacy(sentence)\n",
        "    replaced_sentence = []\n",
        "\n",
        "    for word in words:\n",
        "        # Check if the word contains any forbidden character\n",
        "        if any(char in word for char in forbidden_chars):\n",
        "            # Mask the word in the sentence\n",
        "            masked_sentence = sentence.replace(word, \"[MASK]\", 1)\n",
        "\n",
        "            # Tokenize and input the masked sentence into the model\n",
        "            inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
        "\n",
        "            # Make predictions using the model\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "            # Get the predicted token\n",
        "            mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
        "            predictions = outputs.logits[0, mask_token_index]\n",
        "\n",
        "            # Choose the predicted token that doesn't contain forbidden characters\n",
        "            predicted_token = None\n",
        "            for _ in range(predictions.size(-1)):\n",
        "                predicted_token_id = torch.argmax(predictions, dim=-1).item()\n",
        "                predicted_token = tokenizer.decode([predicted_token_id])\n",
        "\n",
        "                # Check if the predicted token contains forbidden characters\n",
        "                if not any(char in predicted_token for char in forbidden_chars):\n",
        "                    break\n",
        "                else:\n",
        "                    # Remove the invalid token if it contains forbidden characters\n",
        "                    predictions[:, predicted_token_id] = -float(\"inf\")  # Mask the invalid token\n",
        "\n",
        "            # If no valid token found, use the original word\n",
        "            if not predicted_token or any(char in predicted_token for char in forbidden_chars):\n",
        "                replaced_sentence.append(word)\n",
        "            else:\n",
        "                replaced_sentence.append(predicted_token)\n",
        "        else:\n",
        "            replaced_sentence.append(word)  # Keep the original word if it doesn't contain forbidden characters\n",
        "\n",
        "    return \" \".join(replaced_sentence)\n",
        "\n",
        "# Initialize the BERT model and tokenizer for English\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"yesterday evening, i walked along the beach, listening to the sound of the waves and feeling the cool breeze on my face. the sunset painted the sky with shades of orange and pink, creating a breathtaking view. later, i met my family for dinner at a seafood restaurant, where we shared stories and enjoyed delicious dishes together. it was a peaceful and memorable evening.\"\n",
        "\n",
        "# Forbidden characters (example: vowels including \"o\" and \"i\")\n",
        "forbidden_chars = [\"e\", \"a\"]\n",
        "\n",
        "# Apply the lipogram transformation\n",
        "new_sentence = lipogram_sentence_english(sentence, model, tokenizer, forbidden_chars)\n",
        "\n",
        "print(\"\\nOriginal sentence:\\n\", sentence)\n",
        "print(\"\\nLipogrammed sentence:\\n\", new_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bEmxapxyBbbr",
        "outputId": "f723ae13-ff12-432a-e964-cec95e791d78"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "\n",
        "# BERT モデルとトークナイザーをロード\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def lipogram_sentence_english(sentence, model, tokenizer, forbidden_chars, top_k=10):\n",
        "    \"\"\"\n",
        "    指定された禁止文字を含む単語を BERT により適切な単語に置換し、自然なリポグラム文を生成する。\n",
        "\n",
        "    :param sentence: 元の文章\n",
        "    :param model: BERT 言語モデル\n",
        "    :param tokenizer: BERT トークナイザー\n",
        "    :param forbidden_chars: 禁止する文字のリスト\n",
        "    :param top_k: 予測する上位候補の数\n",
        "    :return: リポグラム化された文章\n",
        "    \"\"\"\n",
        "    words = sentence.split()  # 単純なスペース区切りで単語を取得\n",
        "    replaced_words = []\n",
        "\n",
        "    for word in words:\n",
        "        # 禁止文字が含まれるかチェック\n",
        "        if any(char in word for char in forbidden_chars):\n",
        "            # [MASK] で置換\n",
        "            masked_sentence = sentence.replace(word, \"[MASK]\", 1)\n",
        "            inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
        "\n",
        "            # 予測実行\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "            mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
        "            predictions = outputs.logits[0, mask_token_index]\n",
        "\n",
        "            # top_k 個の候補を取得\n",
        "            top_k_tokens = torch.topk(predictions, top_k).indices[0].tolist()\n",
        "\n",
        "            # 禁止文字を含まない最適な候補を選択\n",
        "            predicted_token = word  # 初期値は元の単語\n",
        "            for token_id in top_k_tokens:\n",
        "                candidate = tokenizer.decode([token_id])\n",
        "                if not any(char in candidate for char in forbidden_chars):\n",
        "                    predicted_token = candidate\n",
        "                    break\n",
        "\n",
        "            replaced_words.append(predicted_token)  # 置換後の単語をリストに追加\n",
        "        else:\n",
        "            replaced_words.append(word)  # 禁止文字を含まない場合は元の単語を保持\n",
        "\n",
        "    return \" \".join(replaced_words)\n",
        "\n",
        "# 入力文\n",
        "original_sentence = \"yesterday evening, i walked along the beach, listening to the sound of the waves and feeling the cool breeze on my face.\"\n",
        "# original_sentence = \"yesterday evening, i walked along the beach, listening to the sound of the waves and feeling the cool breeze on my face. the sunset painted the sky with shades of orange and pink, creating a breathtaking view. later, i met my family for dinner at a seafood restaurant, where we shared stories and enjoyed delicious dishes together. it was a peaceful and memorable evening.\"\n",
        "\n",
        "# 禁止文字\n",
        "forbidden_chars = [\"a\"]\n",
        "\n",
        "# 変換実行\n",
        "lipogrammed_sentence = lipogram_sentence_english(original_sentence, model, tokenizer, forbidden_chars)\n",
        "\n",
        "print(\"\\nOriginal sentence:\\n\", original_sentence)\n",
        "print(\"\\nLipogrammed sentence:\\n\", lipogrammed_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EJkEUZkCiqm"
      },
      "source": [
        "###評価してみた"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LULw2h9SCltB",
        "outputId": "b761dc21-62eb-4216-d309-57e7638f39a5"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# モデルのロード\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# 比較する文章\n",
        "# sentence1 = sentence\n",
        "# sentence2 = new_sentence\n",
        "sentence1 = original_sentence\n",
        "sentence2 = lipogrammed_sentence\n",
        "\n",
        "# 文章をベクトルに変換\n",
        "embeddings1 = model.encode(sentence1, convert_to_tensor=True)\n",
        "embeddings2 = model.encode(sentence2, convert_to_tensor=True)\n",
        "\n",
        "# コサイン類似度の計算\n",
        "cosine_score = util.pytorch_cos_sim(embeddings1, embeddings2)[0][0]\n",
        "\n",
        "print(f\"文章1と文章2の類似度: {cosine_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It5nSb3wm6Hi"
      },
      "source": [
        "#新バージョン"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRsTnj_qzZyA",
        "outputId": "284209a1-801f-4c87-e41b-22407fc90bf1"
      },
      "outputs": [],
      "source": [
        "!apt install -y mecab libmecab-dev mecab-ipadic-utf8\n",
        "!pip uninstall -y fugashi unidic_lite\n",
        "!pip install fugashi[unidic-lite]\n",
        "\n",
        "!pip install fugashi\n",
        "!pip install ipadic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8Y2q0-Y70Gg"
      },
      "outputs": [],
      "source": [
        "openai.api_key = \"your-api-key\"  # ご自身のAPIキーをここに設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKf85h1ul_gm",
        "outputId": "602dcdb9-6177-4a75-9f08-61690ef58f9a"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from fugashi import Tagger\n",
        "import random\n",
        "\n",
        "MODEL_NAME = \"gpt-4\"\n",
        "\n",
        "banned_chars = [\"さ\", \"い\"]  # 禁止文字\n",
        "\n",
        "tagger = Tagger()\n",
        "\n",
        "def katakana_to_hiragana(katakana):\n",
        "    return ''.join(chr(ord(c) - 0x60) if 'ァ' <= c <= 'ン' else c for c in katakana)\n",
        "\n",
        "def get_reading(token):\n",
        "    try:\n",
        "        # トークンの読み仮名（カナ）を取得\n",
        "        reading = token.feature.kana\n",
        "        if reading != '*' and reading:\n",
        "            return katakana_to_hiragana(reading)\n",
        "    except Exception as e:\n",
        "        print(f\"エラー: {e}\")\n",
        "\n",
        "    # 読み仮名が取得できなかった場合は、元の表層形を基に変換\n",
        "    return katakana_to_hiragana(token.surface)\n",
        "\n",
        "def get_pos(token):\n",
        "    try:\n",
        "        # 品詞がリストの0番目にある場合（Unidic）\n",
        "        pos = token.feature.pos1\n",
        "        if pos != '*' and pos:\n",
        "            return pos\n",
        "    except Exception as e:\n",
        "        print(f\"エラー: {e}\")\n",
        "    return \"名詞\"  # デフォルトとして名詞を返す\n",
        "\n",
        "def contains_banned(text):\n",
        "    return any(c in text for c in banned_chars)\n",
        "\n",
        "def rewrite_token(original_word, context=None, previous_replacements=None, pos=None, max_attempts=5):\n",
        "    for attempt in range(max_attempts):\n",
        "        # OpenAI APIで新しい単語候補を生成\n",
        "        prompt = f\"\"\"\n",
        "        以下の単語「{original_word}」は、禁止文字「{'、'.join(banned_chars)}」を含むため、文脈に合った自然な表現に**単語単位**で言い換えてください。\n",
        "        ・文の文脈：「{context}」\n",
        "        ・対象の単語：「{original_word}」\n",
        "        ・品詞：「{pos}」\n",
        "        ・出力は置き換えた語句 **一単語** にしてください。\n",
        "        ・出力する語句は必ず**{original_word}**と違う単語を出力して\n",
        "        ・禁止文字「{'、'.join(banned_chars)}」を**表記にも読み（ひらがな）にも含まないこと**。\n",
        "        \"\"\"\n",
        "\n",
        "        # OpenAI APIへのリクエスト（新しいインターフェース）\n",
        "        response = openai.chat.completions.create(\n",
        "            model=MODEL_NAME,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.5,  # 少しランダム性を持たせる\n",
        "            max_tokens=100\n",
        "        )\n",
        "\n",
        "        # 生成された候補\n",
        "        candidate = response.choices[0].message.content.strip()\n",
        "\n",
        "        # トークン化して読み仮名を取得する\n",
        "        temp_token = list(tagger(candidate))[0]  # 変換後の単語を再トークン化\n",
        "        candidate_reading = get_reading(temp_token)\n",
        "\n",
        "        # 禁止文字が含まれていない場合に結果を返す\n",
        "        if not contains_banned(candidate) and not contains_banned(candidate_reading):\n",
        "            return candidate, candidate_reading\n",
        "\n",
        "    # 変換候補が見つからない場合、元の単語を返す\n",
        "    return original_word, get_reading(list(tagger(original_word))[0])\n",
        "\n",
        "def rewrite_sentence_with_reading_check(sentence):\n",
        "    tokens = list(tagger(sentence))\n",
        "    new_tokens = []\n",
        "    previous_replacements = set()  # 履歴を保持\n",
        "\n",
        "    for token in tokens:\n",
        "        surface = token.surface\n",
        "        reading = get_reading(token)\n",
        "        pos = get_pos(token)\n",
        "\n",
        "        print(token)\n",
        "\n",
        "        print(f\"トークン：{surface}（読み：{reading}）\")\n",
        "\n",
        "        if contains_banned(surface) or contains_banned(reading):\n",
        "            print(f\"❌ 禁止文字を含む：{surface}（読み：{reading}）\")\n",
        "            # 置き換えを行う\n",
        "            replacement, replacement_reading = rewrite_token(surface, sentence, previous_replacements=previous_replacements, pos=pos)\n",
        "\n",
        "            # 置き換えた後に再度禁止文字に引っかかるかチェック\n",
        "            while contains_banned(replacement) or contains_banned(replacement_reading):\n",
        "                replacement, replacement_reading = rewrite_token(replacement, sentence, previous_replacements=previous_replacements, pos=pos)\n",
        "\n",
        "            print(f\"👉 「{surface}」→「{replacement}」（読み：{replacement_reading}）\")\n",
        "            new_tokens.append(replacement)\n",
        "        else:\n",
        "            new_tokens.append(surface)\n",
        "\n",
        "    return ''.join(new_tokens)\n",
        "\n",
        "# 🔵 テスト実行\n",
        "input_text = \"さるも木から落ちる。犬も歩けば棒に当たる\"\n",
        "print(\"🔵 元の文：\", input_text)\n",
        "output_text = rewrite_sentence_with_reading_check(input_text)\n",
        "print(\"🟢 変換後：\", output_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDMSyMcr7cwx",
        "outputId": "e3ceef41-5af4-41ca-9f1a-b996aeac00d4"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from fugashi import Tagger\n",
        "import random\n",
        "import re\n",
        "\n",
        "MODEL_NAME = \"gpt-4\"\n",
        "\n",
        "# 禁止文字（表記・読み両方から排除）\n",
        "banned_chars = [\"さ\", \"い\"]\n",
        "\n",
        "# 形態素解析器（Unidic 推奨）\n",
        "tagger = Tagger()\n",
        "\n",
        "# カタカナ→ひらがな変換\n",
        "def katakana_to_hiragana(katakana):\n",
        "    return ''.join(chr(ord(c) - 0x60) if 'ァ' <= c <= 'ン' else c for c in katakana)\n",
        "\n",
        "# 読み仮名取得\n",
        "def get_reading(token):\n",
        "    try:\n",
        "        # Unidic の場合 feature.kana\n",
        "        reading = getattr(token.feature, \"kana\", None) or getattr(token.feature, \"reading\", None)\n",
        "        if reading and reading != \"*\":\n",
        "            return katakana_to_hiragana(reading)\n",
        "    except Exception as e:\n",
        "        print(f\"[読み取得エラー] {token.surface} : {e}\")\n",
        "    return katakana_to_hiragana(token.surface)\n",
        "\n",
        "# 品詞取得（Unidic を前提とする）\n",
        "def get_pos(token):\n",
        "    try:\n",
        "        pos = getattr(token.feature, \"pos1\", None)\n",
        "        return pos if pos and pos != \"*\" else \"名詞\"\n",
        "    except Exception as e:\n",
        "        print(f\"[品詞取得エラー] {token.surface} : {e}\")\n",
        "    return \"名詞\"\n",
        "\n",
        "# 禁止文字を含むか\n",
        "def contains_banned(text):\n",
        "    return any(c in text for c in banned_chars)\n",
        "\n",
        "# GPTによる言い換え生成（単語単位）\n",
        "def rewrite_token(original_word, context=None, previous_replacements=None, pos=None, max_attempts=5):\n",
        "    for attempt in range(max_attempts):\n",
        "        prompt = f\"\"\"\n",
        "以下の単語「{original_word}」は、禁止文字「{'、'.join(banned_chars)}」を含むため、文脈に合った自然な表現に**単語単位**で言い換えてください。\n",
        "・文の文脈：「{context}」\n",
        "・対象の単語：「{original_word}」\n",
        "・品詞：「{pos}」\n",
        "・出力は置き換えた語句 **一単語のみ** にしてください。\n",
        "・絶対に説明文や補足は付けず、単語だけを出力してください。\n",
        "・出力する語句は必ず**{original_word}**と異なる新しい単語であること。\n",
        "・禁止文字「{'、'.join(banned_chars)}」を**表記にも読み（ひらがな）にも含まないこと**。\n",
        "\"\"\"\n",
        "\n",
        "        response = openai.chat.completions.create(\n",
        "            model=MODEL_NAME,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.5,\n",
        "            max_tokens=100\n",
        "        )\n",
        "\n",
        "        candidate = response.choices[0].message.content.strip()\n",
        "\n",
        "        # 不要な記号を削除し、一語だけ抽出\n",
        "        candidate = re.sub(r'[「」『』\"\\'（）()［］\\[\\]]', '', candidate).split()[0]\n",
        "\n",
        "        temp_token = list(tagger(candidate))[0]\n",
        "        candidate_reading = get_reading(temp_token)\n",
        "\n",
        "        if not contains_banned(candidate) and not contains_banned(candidate_reading):\n",
        "            return candidate, candidate_reading\n",
        "\n",
        "    # 失敗した場合は元の単語を返す\n",
        "    fallback_token = list(tagger(original_word))[0]\n",
        "    return original_word, get_reading(fallback_token)\n",
        "\n",
        "# 文を処理し、禁止文字を含む単語を差し替える\n",
        "def rewrite_sentence_with_reading_check(sentence):\n",
        "    tokens = list(tagger(sentence))\n",
        "    new_tokens = []\n",
        "    previous_replacements = set()\n",
        "\n",
        "    for token in tokens:\n",
        "        surface = token.surface\n",
        "        reading = get_reading(token)\n",
        "        pos = get_pos(token)\n",
        "\n",
        "        print(f\"トークン：{surface}（読み：{reading}）\")\n",
        "\n",
        "        if contains_banned(surface) or contains_banned(reading):\n",
        "            print(f\"❌ 禁止文字を含む：{surface}（読み：{reading}）\")\n",
        "            replacement, replacement_reading = rewrite_token(surface, sentence, previous_replacements, pos)\n",
        "\n",
        "            # 再チェック（最大5回）\n",
        "            retry_count = 0\n",
        "            while (contains_banned(replacement) or contains_banned(replacement_reading)) and retry_count < 5:\n",
        "                replacement, replacement_reading = rewrite_token(replacement, sentence, previous_replacements, pos)\n",
        "                retry_count += 1\n",
        "\n",
        "            print(f\"👉 「{surface}」→「{replacement}」（読み：{replacement_reading}）\")\n",
        "            new_tokens.append(replacement)\n",
        "        else:\n",
        "            new_tokens.append(surface)\n",
        "\n",
        "    return ''.join(new_tokens)\n",
        "\n",
        "# 🔵 テスト実行\n",
        "input_text = \"さるも木から落ちる。犬も歩けば棒に当たる\"\n",
        "print(\"🔵 元の文：\", input_text)\n",
        "output_text = rewrite_sentence_with_reading_check(input_text)\n",
        "print(\"🟢 変換後：\", output_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gorzxH151VBW",
        "outputId": "4f252446-2bc0-48eb-e741-4330e6ba495c"
      },
      "outputs": [],
      "source": [
        "from fugashi import Tagger\n",
        "\n",
        "def katakana_to_hiragana(katakana):\n",
        "    return ''.join([chr(ord(c) - 0x60) if 'ァ' <= c <= 'ン' else c for c in katakana])\n",
        "\n",
        "def get_reading(token):\n",
        "    # Unidic では読みは feature[7] にあります（ない場合は表層形を返す）\n",
        "    features = token.feature\n",
        "    if (features):\n",
        "        reading = features[6] #読み仮名を代入\n",
        "        print(features)\n",
        "        if reading != \"*\" and reading:\n",
        "            return katakana_to_hiragana(reading)\n",
        "    return token.surface\n",
        "\n",
        "# Unidic Lite を使う\n",
        "tagger = Tagger(\"-d /usr/local/lib/python3.11/dist-packages/unidic_lite/dicdir\")\n",
        "\n",
        "sentence = \"犬も歩けば棒に当たる\"\n",
        "for token in tagger(sentence):\n",
        "    print(f\"表層形：{token.surface}　読み：{get_reading(token)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glYZbbQU-P6D",
        "outputId": "24a63ff4-49ea-47aa-a8f0-f45ecb1f134d"
      },
      "outputs": [],
      "source": [
        "!apt install -y mecab libmecab-dev mecab-ipadic-utf8\n",
        "!pip install mecab-python3 fugashi ipadic\n",
        "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git\n",
        "!echo yes | mecab-ipadic-neologd/bin/install.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4zT9qGzLRFe",
        "outputId": "f36222e4-7932-4731-c5a3-31d094ed6b89"
      },
      "outputs": [],
      "source": [
        "import fugashi\n",
        "import ipadic\n",
        "\n",
        "tagger = fugashi.GenericTagger(ipadic.MECAB_ARGS)\n",
        "text = \"私はAIが好きです\"\n",
        "\n",
        "for word in tagger(text):\n",
        "    surface = word.surface\n",
        "    features = word.feature\n",
        "    print(f\"{surface}: {features}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "TPBXdYdi_HyB",
        "pyP0RRyd7raX",
        "_EJkEUZkCiqm"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XWs-3PSSomMG",
        "outputId": "1b7d780a-7ac6-4085-e9a6-bd057559d984"
      },
      "outputs": [],
      "source": [
        "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!pip uninstall mecab-python3 fugashi\n",
        "!pip install mecab-python3==1.0.5 fugashi[unidic-lite]\n",
        "!pip install transformers\n",
        "!pip install jaconv\n",
        "!pip install ginza\n",
        "!pip install spacy\n",
        "!python -m spacy download ja_ginza\n",
        "!pip install sentence-transformers\n",
        "!python -m spacy download ja_core_news_sm\n",
        "!pip install janome\n",
        "!pip install mojimoji\n",
        "!pip install openai fugashi unidic-lite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPBXdYdi_HyB"
      },
      "source": [
        "##æ—¥æœ¬èªã§ã‚„ã£ã¦ã¿ãŸï¼ˆæœªå®Œæˆï¼‰"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDzdv_pesngM"
      },
      "source": [
        "###NLTKãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDBSUQqVsTIq",
        "outputId": "80d70b0c-d21c-4231-e94f-f3c31b776f07"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "# NLTKãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zenJsUFPsr7w"
      },
      "source": [
        "###æ—¥æœ¬èªBERTãƒ¢ãƒ‡ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hk9nd7AxoxgR",
        "outputId": "7e4a1bf5-ef92-4080-b239-bfd9b05228a3"
      },
      "outputs": [],
      "source": [
        "# å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "from transformers import pipeline\n",
        "\n",
        "# æ—¥æœ¬èªBERTãƒ¢ãƒ‡ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
        "def setup_model():\n",
        "    return pipeline(\"fill-mask\", model=\"cl-tohoku/bert-base-japanese\")\n",
        "\n",
        "model = setup_model()\n",
        "print(\"ãƒ¢ãƒ‡ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi5ttDBMswwj"
      },
      "source": [
        "###Fugashiã‚’ç”¨ã„ãŸæ—¥æœ¬èªã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xrq0wx6sbfR"
      },
      "outputs": [],
      "source": [
        "from fugashi import Tagger\n",
        "\n",
        "# æ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
        "tagger = Tagger()\n",
        "\n",
        "def tokenize_japanese(sentence):\n",
        "    return [word.surface for word in tagger(sentence)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i12ogSq1Ed8"
      },
      "source": [
        "###ã²ã‚‰ãŒãªã«å¤‰æ›ã™ã‚‹å‡¦ç†ã®è¿½åŠ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCedPLOV1LHM"
      },
      "outputs": [],
      "source": [
        "import jaconv\n",
        "\n",
        "def convert_to_hiragana(sentence):\n",
        "    # æ¼¢å­—ã‚„ã‚«ã‚¿ã‚«ãƒŠã‚’ã²ã‚‰ãŒãªã«å¤‰æ›\n",
        "    return jaconv.hira2kata(jaconv.kata2hira(sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yotac7Wms3sM"
      },
      "source": [
        "###ãƒªãƒã‚°ãƒ©ãƒ åŒ–å‡¦ç†ã®é–¢æ•°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gSXgXtP4fMf",
        "outputId": "14d2c5d1-2016-4765-ec04-249e27ee486c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import re\n",
        "\n",
        "# BERTãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æº–å‚™\n",
        "tokenizer = BertTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
        "model = BertForMaskedLM.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
        "\n",
        "# ç¦æ­¢ã•ã‚ŒãŸæ–‡å­—ã‚’å«ã‚€å˜èªã‚’ç½®ãæ›ãˆã‚‹é–¢æ•°\n",
        "def lipogram_sentence_dev(sentence, model, forbidden_chars):\n",
        "    \"\"\"\n",
        "    ç‰¹å®šã®æ–‡å­—ã‚’å«ã‚€å˜èªã‚’ç‰¹å®šã®æ–‡å­—ã‚’å«ã¾ãªã„å˜èªã«ç½®ãæ›ãˆã‚‹\n",
        "    :param sentence: å…¥åŠ›æ–‡\n",
        "    :param model: BERTãƒ¢ãƒ‡ãƒ«\n",
        "    :param forbidden_chars: å«ã‚“ã§ã¯ã„ã‘ãªã„æ–‡å­—ã®ãƒªã‚¹ãƒˆ\n",
        "    :return: ãƒªãƒã‚°ãƒ©ãƒ åŒ–ã•ã‚ŒãŸæ–‡\n",
        "    \"\"\"\n",
        "    # æ–‡ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
        "    words = sentence.split()  # ç°¡æ˜“çš„ãªåˆ†å‰²ã€å¿…è¦ã«å¿œã˜ã¦å½¢æ…‹ç´ è§£æã‚’ä½¿ã†\n",
        "\n",
        "    replaced_sentence = []\n",
        "\n",
        "    for word in words:\n",
        "        # ç¦æ­¢ã•ã‚ŒãŸæ–‡å­—ã‚’å«ã‚€å˜èªã‚’ãƒã‚§ãƒƒã‚¯\n",
        "        if any(char in word for char in forbidden_chars):\n",
        "            # å˜èªã‚’ãƒã‚¹ã‚¯\n",
        "            masked_sentence = sentence.replace(word, \"[MASK]\")\n",
        "\n",
        "            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
        "            inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "            # äºˆæ¸¬ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®ç¢ºç‡ã‚’å–å¾—\n",
        "            predictions = outputs.logits[0, inputs['input_ids'][0] == tokenizer.mask_token_id]\n",
        "            predicted_token_id = torch.argmax(predictions).item()\n",
        "\n",
        "            # äºˆæ¸¬ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—\n",
        "            predicted_token = tokenizer.decode([predicted_token_id])\n",
        "\n",
        "            # ç¦æ­¢ã•ã‚ŒãŸæ–‡å­—ã‚’å«ã¾ãªã„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¸æŠ\n",
        "            if not any(char in predicted_token for char in forbidden_chars):\n",
        "                replaced_sentence.append(predicted_token)\n",
        "            else:\n",
        "                replaced_sentence.append(word)  # é©åˆ‡ãªå€™è£œãŒãªã‘ã‚Œã°å…ƒã®å˜èªã‚’ä½¿ã†\n",
        "        else:\n",
        "            # ç¦æ­¢ã•ã‚ŒãŸæ–‡å­—ã‚’å«ã¾ãªã„å˜èªã¯ãã®ã¾ã¾ä½¿ç”¨\n",
        "            replaced_sentence.append(word)\n",
        "\n",
        "    return \" \".join(replaced_sentence)\n",
        "\n",
        "# å®Ÿè¡Œä¾‹\n",
        "sentence = \"ã‚ãŸã—ã¯ãã®ã†ã®ã”ã”ã€ãŒã£ã“ã†ã§ã¨ã‚‚ã ã¡ã¨ãŸã®ã—ã„ã˜ã‹ã‚“ã‚’ã™ã”ã—ã¾ã—ãŸã€‚\"\n",
        "forbidden_chars = ['ã‹', 'ã¤']  # ç½®ãæ›ãˆå¯¾è±¡ã®èªå½™\n",
        "\n",
        "result = lipogram_sentence_dev(sentence, model, forbidden_chars)\n",
        "print(\"ãƒªãƒã‚°ãƒ©ãƒ åŒ–å¾Œ:\", result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbVsMnI18mwW",
        "outputId": "87c8b348-748c-422f-a304-42fa9238b5fa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import spacy\n",
        "\n",
        "# SpaCyæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰\n",
        "nlp = spacy.load(\"ja_core_news_sm\")\n",
        "\n",
        "# BERTãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æº–å‚™\n",
        "tokenizer = BertTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
        "model = BertForMaskedLM.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
        "\n",
        "# ç¦æ­¢ã•ã‚ŒãŸæ–‡å­—ã‚’å«ã‚€å˜èªã‚’ç½®ãæ›ãˆã‚‹é–¢æ•°\n",
        "def lipogram_sentence_dev(sentence, model, tokenizer, forbidden_chars):\n",
        "    \"\"\"\n",
        "    ç‰¹å®šã®æ–‡å­—ã‚’å«ã‚€å˜èªã‚’ç‰¹å®šã®æ–‡å­—ã‚’å«ã¾ãªã„å˜èªã«ç½®ãæ›ãˆã‚‹\n",
        "    :param sentence: å…¥åŠ›æ–‡\n",
        "    :param model: BERTãƒ¢ãƒ‡ãƒ«\n",
        "    :param tokenizer: BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼\n",
        "    :param forbidden_chars: å«ã‚“ã§ã¯ã„ã‘ãªã„æ–‡å­—ã®ãƒªã‚¹ãƒˆ\n",
        "    :return: ãƒªãƒã‚°ãƒ©ãƒ åŒ–ã•ã‚ŒãŸæ–‡\n",
        "    \"\"\"\n",
        "    # å½¢æ…‹ç´ è§£æã§å˜èªã«åˆ†å‰²\n",
        "    doc = nlp(sentence)\n",
        "    words = [token.text for token in doc]\n",
        "\n",
        "    replaced_sentence = []\n",
        "\n",
        "    for word in words:\n",
        "        # ç¦æ­¢ã•ã‚ŒãŸæ–‡å­—ã‚’å«ã‚€å˜èªã‚’ãƒã‚§ãƒƒã‚¯\n",
        "        if any(char in word for char in forbidden_chars):\n",
        "            # å˜èªã‚’ãƒã‚¹ã‚¯\n",
        "            masked_sentence = sentence.replace(word, \"[MASK]\", 1)\n",
        "\n",
        "            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
        "            inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "            # äºˆæ¸¬ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®ç¢ºç‡ã‚’å–å¾—\n",
        "            mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
        "            predictions = outputs.logits[0, mask_token_index]\n",
        "\n",
        "            # ä¸Šä½å€™è£œã‚’å–å¾—\n",
        "            top_k = 10  # ä¸Šä½10å€™è£œã‚’å–å¾—\n",
        "            top_k_ids = torch.topk(predictions, k=top_k, dim=-1).indices[0].tolist()\n",
        "            candidates = [tokenizer.decode([idx]).strip() for idx in top_k_ids]\n",
        "\n",
        "            # ç¦æ­¢ã•ã‚ŒãŸæ–‡å­—ã‚’å«ã¾ãªã„å˜èªã‚’é¸æŠ\n",
        "            replacement = next((candidate for candidate in candidates if not any(char in candidate for char in forbidden_chars)), word)\n",
        "\n",
        "            replaced_sentence.append(replacement)\n",
        "        else:\n",
        "            # ç¦æ­¢ã•ã‚ŒãŸæ–‡å­—ã‚’å«ã¾ãªã„å˜èªã¯ãã®ã¾ã¾ä½¿ç”¨\n",
        "            replaced_sentence.append(word)\n",
        "\n",
        "    return \"\".join(replaced_sentence)  # æ—¥æœ¬èªã¯å˜èªé–“ã«ã‚¹ãƒšãƒ¼ã‚¹ã‚’æŒ¿å…¥ã—ãªã„\n",
        "\n",
        "# å®Ÿè¡Œä¾‹\n",
        "sentence = \"ã‚ãŸã—ã¯ãã®ã†ã®ã”ã”ã€ãŒã£ã“ã†ã§ã¨ã‚‚ã ã¡ã¨ãŸã®ã—ã„ã˜ã‹ã‚“ã‚’ã™ã”ã—ã¾ã—ãŸã€‚\"\n",
        "# sentence = \"ç§ã¯æ˜¨æ—¥ã®åˆå¾Œã€å­¦æ ¡ã§å‹é”ã¨æ¥½ã—ã„æ™‚é–“ã‚’éã”ã—ã¾ã—ãŸã€‚\"\n",
        "forbidden_chars = ['ã‹', 'ã¤', 'ã¿', 'ã®']  # ç½®ãæ›ãˆå¯¾è±¡ã®èªå½™\n",
        "\n",
        "result = lipogram_sentence_dev(sentence, model, tokenizer, forbidden_chars)\n",
        "print(\"ãƒªãƒã‚°ãƒ©ãƒ åŒ–å¾Œ:\", result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "MGAtV5SbDNSj",
        "outputId": "da81191c-a77b-4e9e-eab0-6de7fe0bca14"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
        "from janome.tokenizer import Tokenizer\n",
        "\n",
        "# BERTãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æº–å‚™\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
        "model = BertForMaskedLM.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
        "\n",
        "# Janomeã‚’ä½¿ã£ã¦å˜èªã”ã¨ã«åˆ†å‰²ã™ã‚‹é–¢æ•°\n",
        "def split_into_words(sentence):\n",
        "    tokenizer = Tokenizer()\n",
        "    return [token.surface for token in tokenizer.tokenize(sentence)]\n",
        "\n",
        "# ãƒªãƒã‚°ãƒ©ãƒ ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦å¾Œå‡¦ç†\n",
        "def apply_lipogram_rules(word):\n",
        "    # å°æ–‡å­—ã‚’å¤§æ–‡å­—ã«ã€æ¿éŸ³ãƒ»åŠæ¿éŸ³ã‚’æ¸…éŸ³ã«å¤‰æ›\n",
        "    mapping = {\n",
        "        'ãŒ': 'ã‹', 'ã': 'ã', 'ã': 'ã', 'ã’': 'ã‘', 'ã”': 'ã“',\n",
        "        'ã–': 'ã•', 'ã˜': 'ã—', 'ãš': 'ã™', 'ãœ': 'ã›', 'ã': 'ã',\n",
        "        'ã ': 'ãŸ', 'ã¢': 'ã¡', 'ã¥': 'ã¤', 'ã§': 'ã¦', 'ã©': 'ã¨',\n",
        "        'ã°': 'ã¯', 'ã³': 'ã²', 'ã¶': 'ãµ', 'ã¹': 'ã¸', 'ã¼': 'ã»',\n",
        "        'ã±': 'ã¯', 'ã´': 'ã²', 'ã·': 'ãµ', 'ãº': 'ã¸', 'ã½': 'ã»',\n",
        "        'ãƒ': 'ã„', 'ã‡': 'ãˆ', 'ã…': 'ã†', 'ã‰': 'ãŠ', 'ã£': 'ã¤', 'ãƒ¼': 'ã†'\n",
        "    }\n",
        "    for old, new in mapping.items():\n",
        "        word = word.replace(old, new)\n",
        "\n",
        "    return word\n",
        "\n",
        "def mask_forbidden_characters_words(sentence, forbidden_chars):\n",
        "    \"\"\"\n",
        "    ç¦æ­¢æ–‡å­—ã‚’å«ã‚€å˜èªã‚’[MASK]ã«ç½®ãæ›ãˆã‚‹\n",
        "    \"\"\"\n",
        "    words = split_into_words(sentence)\n",
        "    masked_sentence = []\n",
        "\n",
        "    for word in words:\n",
        "        # ç¦æ­¢æ–‡å­—ã‚’å«ã‚€å˜èªã‚’[MASK]ã«ç½®ãæ›ãˆ\n",
        "        if any(char in word for char in forbidden_chars):\n",
        "            masked_sentence.append(\"[MASK]\")\n",
        "        else:\n",
        "            masked_sentence.append(word)\n",
        "\n",
        "    return \" \".join(masked_sentence)\n",
        "\n",
        "def predict_replacement(sentence, model, tokenizer, forbidden_chars):\n",
        "    \"\"\"\n",
        "    BERTãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦[MASK]ã‚’äºˆæ¸¬ã—ã€ç¦æ­¢æ–‡å­—ã‚’ç½®ãæ›ãˆã‚‹\n",
        "    \"\"\"\n",
        "    masked_sentence = mask_forbidden_characters_words(sentence, forbidden_chars)\n",
        "    inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
        "\n",
        "    # ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # [MASK]ãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®ã‚’å–å¾—\n",
        "    mask_token_indices = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
        "    predictions = outputs.logits[0, mask_token_indices]\n",
        "\n",
        "    # å„[MASK]ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç¦æ­¢æ–‡å­—ã‚’å«ã¾ãªã„äºˆæ¸¬ãƒˆãƒ¼ã‚¯ãƒ³ã§ç½®ãæ›ãˆã‚‹\n",
        "    replaced_sentence = masked_sentence\n",
        "    for i, mask_index in enumerate(mask_token_indices):\n",
        "        top_k = 10\n",
        "        predicted_token_ids = torch.topk(predictions[i], top_k).indices.tolist()\n",
        "        for token_id in predicted_token_ids:\n",
        "            predicted_token = tokenizer.decode([token_id]).strip()\n",
        "            if not any(char in predicted_token for char in forbidden_chars):\n",
        "                replaced_sentence = replaced_sentence.replace(\"[MASK]\", predicted_token, 1)\n",
        "                break\n",
        "        else:\n",
        "            replaced_sentence = replaced_sentence.replace(\"[MASK]\", \"\", 1)  # é©åˆ‡ãªå€™è£œãŒãªã„å ´åˆã¯ç©ºæ–‡å­—ã«ã™ã‚‹\n",
        "\n",
        "    # ãƒªãƒã‚°ãƒ©ãƒ ãƒ«ãƒ¼ãƒ«ã‚’é©ç”¨ã—ã¦çµæœã‚’ä¿®æ­£\n",
        "    final_sentence = \" \".join([apply_lipogram_rules(word) for word in replaced_sentence.split()])\n",
        "\n",
        "    return final_sentence\n",
        "\n",
        "# å®Ÿè¡Œä¾‹\n",
        "sentence = \"ã¤ãã‚ˆã®ã‚ˆã‚‹ã«ã„ã¡ã¾ã„ã®ã¦ãŒã¿ã‚’ã‹ãã¾ã—ãŸ\"\n",
        "forbidden_chars = ['ã¤', 'ã‚ˆ', 'ã„', 'ã¦']  # ç¦æ­¢æ–‡å­—\n",
        "\n",
        "# å®Ÿè¡Œ\n",
        "result = predict_replacement(sentence, model, tokenizer, forbidden_chars)\n",
        "print(\"ãƒªãƒã‚°ãƒ©ãƒ åŒ–å¾Œ:\", result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJU-wMvFIULV",
        "outputId": "eae41bfe-ffa7-4313-b531-a7b30b45fa2a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
        "from janome.tokenizer import Tokenizer\n",
        "\n",
        "# BERTãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æº–å‚™\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
        "model = BertForMaskedLM.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
        "\n",
        "# Janomeã‚’ä½¿ã£ã¦å˜èªã”ã¨ã«åˆ†å‰²ã™ã‚‹é–¢æ•°\n",
        "def split_into_words(sentence):\n",
        "    tokenizer = Tokenizer()\n",
        "    return [token.surface for token in tokenizer.tokenize(sentence)]\n",
        "\n",
        "# ç¦æ­¢æ–‡å­—ã‚’å«ã‚€å˜èªã‚’[MASK]ã«ç½®ãæ›ãˆã‚‹é–¢æ•°\n",
        "def mask_forbidden_characters_words(sentence, forbidden_chars):\n",
        "    \"\"\"\n",
        "    ç¦æ­¢æ–‡å­—ã‚’å«ã‚€å˜èªã‚’[MASK]ã«ç½®ãæ›ãˆã‚‹\n",
        "    \"\"\"\n",
        "    words = split_into_words(sentence)\n",
        "    masked_sentence = []\n",
        "    masked_words = []  # å¤‰æ›ã•ã‚ŒãŸå˜èªã‚’è¨˜éŒ²\n",
        "\n",
        "    for word in words:\n",
        "        # ç¦æ­¢æ–‡å­—ã‚’å«ã‚€å˜èªã‚’[MASK]ã«ç½®ãæ›ãˆ\n",
        "        if any(char in word for char in forbidden_chars):\n",
        "            masked_sentence.append(\"[MASK]\")\n",
        "            masked_words.append(word)  # ç½®ãæ›ãˆãŸå˜èªã‚’è¨˜éŒ²\n",
        "        else:\n",
        "            masked_sentence.append(word)\n",
        "\n",
        "    return \" \".join(masked_sentence), masked_words\n",
        "\n",
        "# BERTãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦[MASK]ã‚’äºˆæ¸¬ã—ã€å¤‰æ›å€™è£œã‚’è¡¨ç¤ºã™ã‚‹é–¢æ•°\n",
        "def predict_replacement(sentence, model, tokenizer, forbidden_chars):\n",
        "    \"\"\"\n",
        "    BERTãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦[MASK]ã‚’äºˆæ¸¬ã—ã€ç¦æ­¢æ–‡å­—ã‚’ç½®ãæ›ãˆã‚‹\n",
        "    \"\"\"\n",
        "    masked_sentence, masked_words = mask_forbidden_characters_words(sentence, forbidden_chars)\n",
        "    inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
        "\n",
        "    # ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # [MASK]ãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®ã‚’å–å¾—\n",
        "    mask_token_indices = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
        "    predictions = outputs.logits[0, mask_token_indices]\n",
        "\n",
        "    # å„[MASK]ãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬ã—ã€å¤‰æ›å€™è£œã‚’è¡¨ç¤º\n",
        "    predicted_words = []\n",
        "    for i, mask_index in enumerate(mask_token_indices):\n",
        "        top_k = 10\n",
        "        predicted_token_ids = torch.topk(predictions[i], top_k).indices.tolist()\n",
        "        candidate_tokens = [tokenizer.decode([token_id]).strip() for token_id in predicted_token_ids]\n",
        "        predicted_words.append(candidate_tokens)\n",
        "\n",
        "    return masked_sentence, masked_words, predicted_words\n",
        "\n",
        "# å®Ÿè¡Œä¾‹\n",
        "sentence = \"ã¤ãã‚ˆã®ã‚ˆã‚‹ã«ã„ã¡ã¾ã„ã®ã¦ãŒã¿ã‚’ã‹ãã¾ã—ãŸ\"\n",
        "forbidden_chars = ['ã¤', 'ã‚ˆ', 'ã„', 'ã¦']  # ç¦æ­¢æ–‡å­—\n",
        "\n",
        "# å®Ÿè¡Œ\n",
        "masked_sentence, masked_words, predicted_words = predict_replacement(sentence, model, tokenizer, forbidden_chars)\n",
        "\n",
        "print(\"å…ƒã®æ–‡:\", sentence)\n",
        "print(\"å¤‰æ›å‰ã®æ–‡:\", masked_sentence)\n",
        "print(\"å¤‰æ›ã•ã‚ŒãŸå˜èª:\", masked_words)\n",
        "\n",
        "for i, word in enumerate(masked_words):\n",
        "    print(f\"[MASK]ã«å¤‰æ›ã•ã‚ŒãŸå˜èª: {word}\")\n",
        "    print(f\"å€™è£œ: {predicted_words[i]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAQD0FZwJP44",
        "outputId": "f505730b-67ea-4437-c7b9-ff24ea386a95"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
        "from janome.tokenizer import Tokenizer\n",
        "import mojimoji  # æ¼¢å­—ã‚’ã²ã‚‰ãŒãªã«å¤‰æ›ã™ã‚‹ãŸã‚ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
        "\n",
        "# BERTãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æº–å‚™\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
        "model = BertForMaskedLM.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
        "\n",
        "# Janomeã‚’ä½¿ã£ã¦å˜èªã”ã¨ã«åˆ†å‰²ã™ã‚‹é–¢æ•°\n",
        "def split_into_words(sentence):\n",
        "    tokenizer = Tokenizer()\n",
        "    return [token.surface for token in tokenizer.tokenize(sentence)]\n",
        "\n",
        "# æ¼¢å­—ã‚’ã²ã‚‰ãŒãªã«å¤‰æ›ã™ã‚‹é–¢æ•°\n",
        "def kanji_to_hiragana(word):\n",
        "    # mojimojiã§ç°¡å˜ãªã²ã‚‰ãŒãªå¤‰æ›ã‚’è¡Œã†ï¼ˆâ€»å®Ÿéš›ã«ã¯ã‚‚ã£ã¨ç²¾åº¦é«˜ã„å¤‰æ›ãŒå¿…è¦ï¼‰\n",
        "    return mojimoji.han_to_zen(word)\n",
        "\n",
        "# ãƒªãƒã‚°ãƒ©ãƒ ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦å¾Œå‡¦ç†\n",
        "def apply_lipogram_rules(word):\n",
        "    # å°æ–‡å­—ã‚’å¤§æ–‡å­—ã«ã€æ¿éŸ³ãƒ»åŠæ¿éŸ³ã‚’æ¸…éŸ³ã«å¤‰æ›\n",
        "    mapping = {\n",
        "        'ãŒ': 'ã‹', 'ã': 'ã', 'ã': 'ã', 'ã’': 'ã‘', 'ã”': 'ã“',\n",
        "        'ã–': 'ã•', 'ã˜': 'ã—', 'ãš': 'ã™', 'ãœ': 'ã›', 'ã': 'ã',\n",
        "        'ã ': 'ãŸ', 'ã¢': 'ã¡', 'ã¥': 'ã¤', 'ã§': 'ã¦', 'ã©': 'ã¨',\n",
        "        'ã°': 'ã¯', 'ã³': 'ã²', 'ã¶': 'ãµ', 'ã¹': 'ã¸', 'ã¼': 'ã»',\n",
        "        'ã±': 'ã¯', 'ã´': 'ã²', 'ã·': 'ãµ', 'ãº': 'ã¸', 'ã½': 'ã»',\n",
        "        'ãƒ': 'ã„', 'ã‡': 'ãˆ', 'ã…': 'ã†', 'ã‰': 'ãŠ', 'ã£': 'ã¤', 'ãƒ¼': 'ã†'\n",
        "    }\n",
        "    for old, new in mapping.items():\n",
        "        word = word.replace(old, new)\n",
        "\n",
        "    return word\n",
        "\n",
        "def mask_forbidden_characters_words(sentence, forbidden_chars):\n",
        "    \"\"\"\n",
        "    ç¦æ­¢æ–‡å­—ã‚’å«ã‚€å˜èªã‚’[MASK]ã«ç½®ãæ›ãˆã‚‹\n",
        "    \"\"\"\n",
        "    words = split_into_words(sentence)\n",
        "    masked_sentence = []\n",
        "\n",
        "    for word in words:\n",
        "        # ç¦æ­¢æ–‡å­—ã‚’å«ã‚€å˜èªã‚’[MASK]ã«ç½®ãæ›ãˆ\n",
        "        if any(char in word for char in forbidden_chars):\n",
        "            masked_sentence.append(\"[MASK]\")\n",
        "        else:\n",
        "            masked_sentence.append(word)\n",
        "\n",
        "    return \" \".join(masked_sentence)\n",
        "\n",
        "def predict_replacement(sentence, model, tokenizer, forbidden_chars):\n",
        "    \"\"\"\n",
        "    BERTãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦[MASK]ã‚’äºˆæ¸¬ã—ã€ç¦æ­¢æ–‡å­—ã‚’ç½®ãæ›ãˆã‚‹\n",
        "    \"\"\"\n",
        "    masked_sentence = mask_forbidden_characters_words(sentence, forbidden_chars)\n",
        "    inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
        "\n",
        "    # ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # [MASK]ãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®ã‚’å–å¾—\n",
        "    mask_token_indices = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
        "    predictions = outputs.logits[0, mask_token_indices]\n",
        "\n",
        "    # å„[MASK]ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç¦æ­¢æ–‡å­—ã‚’å«ã¾ãªã„äºˆæ¸¬ãƒˆãƒ¼ã‚¯ãƒ³ã§ç½®ãæ›ãˆã‚‹\n",
        "    replaced_sentence = masked_sentence\n",
        "    for i, mask_index in enumerate(mask_token_indices):\n",
        "        top_k = 10\n",
        "        predicted_token_ids = torch.topk(predictions[i], top_k).indices.tolist()\n",
        "        for token_id in predicted_token_ids:\n",
        "            predicted_token = tokenizer.decode([token_id]).strip()\n",
        "\n",
        "            # æ¼¢å­—ã®å ´åˆã€ã²ã‚‰ãŒãªã«å¤‰æ›\n",
        "            hiragana_predicted = kanji_to_hiragana(predicted_token)\n",
        "\n",
        "            if not any(char in hiragana_predicted for char in forbidden_chars):\n",
        "                replaced_sentence = replaced_sentence.replace(\"[MASK]\", predicted_token, 1)\n",
        "                break\n",
        "        else:\n",
        "            replaced_sentence = replaced_sentence.replace(\"[MASK]\", \"\", 1)  # é©åˆ‡ãªå€™è£œãŒãªã„å ´åˆã¯ç©ºæ–‡å­—ã«ã™ã‚‹\n",
        "\n",
        "    # ãƒªãƒã‚°ãƒ©ãƒ ãƒ«ãƒ¼ãƒ«ã‚’é©ç”¨ã—ã¦çµæœã‚’ä¿®æ­£\n",
        "    final_sentence = \" \".join([apply_lipogram_rules(word) for word in replaced_sentence.split()])\n",
        "\n",
        "    return final_sentence\n",
        "\n",
        "# å®Ÿè¡Œä¾‹\n",
        "sentence = \"ã¤ãã‚ˆã®ã‚ˆã‚‹ã«ã„ã¡ã¾ã„ã®ã¦ãŒã¿ã‚’ã‹ãã¾ã—ãŸ\"\n",
        "forbidden_chars = ['ã¤', 'ã‚ˆ', 'ã„', 'ã¦']  # ç¦æ­¢æ–‡å­—\n",
        "\n",
        "# å®Ÿè¡Œ\n",
        "result = predict_replacement(sentence, model, tokenizer, forbidden_chars)\n",
        "print(\"ãƒªãƒã‚°ãƒ©ãƒ åŒ–å¾Œ:\", result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFBDgeKroyKE"
      },
      "outputs": [],
      "source": [
        "# ãƒªãƒã‚°ãƒ©ãƒ åŒ–å‡¦ç†ã®é–¢æ•°\n",
        "def lipogram_sentence(sentence, model, forbidden_chars):\n",
        "    \"\"\"\n",
        "    ç‰¹å®šã®æ–‡å­—ã‚’å«ã‚€å˜èªã‚’ç‰¹å®šã®æ–‡å­—ã‚’å«ã¾ãªã„å˜èªã«ç½®ãæ›ãˆã‚‹\n",
        "    :param sentence: å…¥åŠ›æ–‡\n",
        "    :param model: BERTãƒ¢ãƒ‡ãƒ«\n",
        "    :param forbidden_chars: å«ã‚“ã§ã¯ã„ã‘ãªã„æ–‡å­—ã®ãƒªã‚¹ãƒˆ\n",
        "    :return: ãƒªãƒã‚°ãƒ©ãƒ åŒ–ã•ã‚ŒãŸæ–‡\n",
        "    \"\"\"\n",
        "    words = tokenize_japanese(sentence)\n",
        "    replaced_sentence = []\n",
        "\n",
        "    for word in words:\n",
        "        # ç¦æ­¢ã•ã‚ŒãŸæ–‡å­—ã‚’å«ã‚€å˜èªã‚’ãƒã‚§ãƒƒã‚¯\n",
        "        if any(char in word for char in forbidden_chars):\n",
        "            # ãƒã‚¹ã‚¯ã•ã‚ŒãŸæ–‡ç« ã‚’ä½œæˆ\n",
        "            masked_sentence = sentence.replace(word, \"[MASK]\", 1)\n",
        "\n",
        "            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¦ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›\n",
        "            inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
        "\n",
        "            # ãƒ¢ãƒ‡ãƒ«ã«äºˆæ¸¬ã‚’ã•ã›ã‚‹\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "            # äºˆæ¸¬ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—\n",
        "            mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
        "            predictions = outputs.logits[0, mask_token_index]\n",
        "\n",
        "            # äºˆæ¸¬ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¸æŠ\n",
        "            predicted_token_id = torch.argmax(predictions, dim=-1).item()\n",
        "            predicted_token = tokenizer.decode([predicted_token_id])\n",
        "\n",
        "            # ç¦æ­¢ã•ã‚ŒãŸæ–‡å­—ã‚’å«ã¾ãªã„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¸æŠ\n",
        "            if not any(char in predicted_token for char in forbidden_chars):\n",
        "                replaced_sentence.append(predicted_token)\n",
        "            else:\n",
        "                replaced_sentence.append(word)  # é©åˆ‡ãªå€™è£œãŒãªã‘ã‚Œã°å…ƒã®å˜èªã‚’ä½¿ã†\n",
        "        else:\n",
        "            replaced_sentence.append(word)  # ç¦æ­¢ã•ã‚ŒãŸæ–‡å­—ã‚’å«ã¾ãªã„å˜èªã¯ãã®ã¾ã¾ä½¿ç”¨\n",
        "\n",
        "    return \"\".join(replaced_sentence)  # æ–‡ç« ã‚’çµåˆã—ã¦è¿”ã™"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "ywczbQfgo25m",
        "outputId": "1022db9d-00b3-4cbb-fe04-b0db21b73ed4"
      },
      "outputs": [],
      "source": [
        "# ãƒ†ã‚¹ãƒˆç”¨ã®æ–‡ç« \n",
        "# sentence = \"ç§ã¯æ˜¨æ—¥ã®åˆå¾Œã€å­¦æ ¡ã§å‹é”ã¨æ¥½ã—ã„æ™‚é–“ã‚’éã”ã—ã¾ã—ãŸã€‚\"\n",
        "sentence = \"ã‚ãŸã—ã¯ãã®ã†ã®ã”ã”ã€ãŒã£ã“ã†ã§ã¨ã‚‚ã ã¡ã¨ãŸã®ã—ã„ã˜ã‹ã‚“ã‚’ã™ã”ã—ã¾ã—ãŸã€‚\"\n",
        "forbidden_chars = [\"ã‹\", \"ã¤\", \"ã¿\", \"ã‚\", \"ã\"]  # ç¦æ­¢æ–‡å­—\n",
        "\n",
        "print(\"å…ƒã®æ–‡:\", sentence)\n",
        "new_sentence = lipogram_sentence(sentence, model, forbidden_chars)\n",
        "print(\"ãƒªãƒã‚°ãƒ©ãƒ åŒ–å¾Œ:\", new_sentence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyP0RRyd7raX"
      },
      "source": [
        "##è‹±èªã§ã‚„ã£ã¦ã¿ãŸ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "o2Asom7v70UD",
        "outputId": "84f1ae66-f33e-4d73-ca97-b81037b62d9f"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import torch\n",
        "\n",
        "# Load English spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to tokenize English text using spaCy\n",
        "def tokenize_english_with_spacy(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    return [token.text for token in doc]\n",
        "\n",
        "# Function to replace words with forbidden characters using a BERT model\n",
        "def lipogram_sentence_english(sentence, model, tokenizer, forbidden_chars):\n",
        "    \"\"\"\n",
        "    Replace words with forbidden characters with alternatives that do not contain forbidden characters.\n",
        "    :param sentence: Input sentence\n",
        "    :param model: BERT model\n",
        "    :param tokenizer: BERT tokenizer\n",
        "    :param forbidden_chars: List of forbidden characters\n",
        "    :return: Lipogrammed sentence\n",
        "    \"\"\"\n",
        "    words = tokenize_english_with_spacy(sentence)\n",
        "    replaced_sentence = []\n",
        "\n",
        "    for word in words:\n",
        "        # Check if the word contains any forbidden character\n",
        "        if any(char in word for char in forbidden_chars):\n",
        "            # Mask the word in the sentence\n",
        "            masked_sentence = sentence.replace(word, \"[MASK]\", 1)\n",
        "\n",
        "            # Tokenize and input the masked sentence into the model\n",
        "            inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
        "\n",
        "            # Make predictions using the model\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "            # Get the predicted token\n",
        "            mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
        "            predictions = outputs.logits[0, mask_token_index]\n",
        "\n",
        "            # Choose the predicted token that doesn't contain forbidden characters\n",
        "            predicted_token = None\n",
        "            for _ in range(predictions.size(-1)):\n",
        "                predicted_token_id = torch.argmax(predictions, dim=-1).item()\n",
        "                predicted_token = tokenizer.decode([predicted_token_id])\n",
        "\n",
        "                # Check if the predicted token contains forbidden characters\n",
        "                if not any(char in predicted_token for char in forbidden_chars):\n",
        "                    break\n",
        "                else:\n",
        "                    # Remove the invalid token if it contains forbidden characters\n",
        "                    predictions[:, predicted_token_id] = -float(\"inf\")  # Mask the invalid token\n",
        "\n",
        "            # If no valid token found, use the original word\n",
        "            if not predicted_token or any(char in predicted_token for char in forbidden_chars):\n",
        "                replaced_sentence.append(word)\n",
        "            else:\n",
        "                replaced_sentence.append(predicted_token)\n",
        "        else:\n",
        "            replaced_sentence.append(word)  # Keep the original word if it doesn't contain forbidden characters\n",
        "\n",
        "    return \" \".join(replaced_sentence)\n",
        "\n",
        "# Initialize the BERT model and tokenizer for English\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"yesterday evening, i walked along the beach, listening to the sound of the waves and feeling the cool breeze on my face. the sunset painted the sky with shades of orange and pink, creating a breathtaking view. later, i met my family for dinner at a seafood restaurant, where we shared stories and enjoyed delicious dishes together. it was a peaceful and memorable evening.\"\n",
        "\n",
        "# Forbidden characters (example: vowels including \"o\" and \"i\")\n",
        "forbidden_chars = [\"e\", \"a\"]\n",
        "\n",
        "# Apply the lipogram transformation\n",
        "new_sentence = lipogram_sentence_english(sentence, model, tokenizer, forbidden_chars)\n",
        "\n",
        "print(\"\\nOriginal sentence:\\n\", sentence)\n",
        "print(\"\\nLipogrammed sentence:\\n\", new_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bEmxapxyBbbr",
        "outputId": "f723ae13-ff12-432a-e964-cec95e791d78"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "\n",
        "# BERT ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def lipogram_sentence_english(sentence, model, tokenizer, forbidden_chars, top_k=10):\n",
        "    \"\"\"\n",
        "    æŒ‡å®šã•ã‚ŒãŸç¦æ­¢æ–‡å­—ã‚’å«ã‚€å˜èªã‚’ BERT ã«ã‚ˆã‚Šé©åˆ‡ãªå˜èªã«ç½®æ›ã—ã€è‡ªç„¶ãªãƒªãƒã‚°ãƒ©ãƒ æ–‡ã‚’ç”Ÿæˆã™ã‚‹ã€‚\n",
        "\n",
        "    :param sentence: å…ƒã®æ–‡ç« \n",
        "    :param model: BERT è¨€èªãƒ¢ãƒ‡ãƒ«\n",
        "    :param tokenizer: BERT ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼\n",
        "    :param forbidden_chars: ç¦æ­¢ã™ã‚‹æ–‡å­—ã®ãƒªã‚¹ãƒˆ\n",
        "    :param top_k: äºˆæ¸¬ã™ã‚‹ä¸Šä½å€™è£œã®æ•°\n",
        "    :return: ãƒªãƒã‚°ãƒ©ãƒ åŒ–ã•ã‚ŒãŸæ–‡ç« \n",
        "    \"\"\"\n",
        "    words = sentence.split()  # å˜ç´”ãªã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§å˜èªã‚’å–å¾—\n",
        "    replaced_words = []\n",
        "\n",
        "    for word in words:\n",
        "        # ç¦æ­¢æ–‡å­—ãŒå«ã¾ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
        "        if any(char in word for char in forbidden_chars):\n",
        "            # [MASK] ã§ç½®æ›\n",
        "            masked_sentence = sentence.replace(word, \"[MASK]\", 1)\n",
        "            inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
        "\n",
        "            # äºˆæ¸¬å®Ÿè¡Œ\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "            mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
        "            predictions = outputs.logits[0, mask_token_index]\n",
        "\n",
        "            # top_k å€‹ã®å€™è£œã‚’å–å¾—\n",
        "            top_k_tokens = torch.topk(predictions, top_k).indices[0].tolist()\n",
        "\n",
        "            # ç¦æ­¢æ–‡å­—ã‚’å«ã¾ãªã„æœ€é©ãªå€™è£œã‚’é¸æŠ\n",
        "            predicted_token = word  # åˆæœŸå€¤ã¯å…ƒã®å˜èª\n",
        "            for token_id in top_k_tokens:\n",
        "                candidate = tokenizer.decode([token_id])\n",
        "                if not any(char in candidate for char in forbidden_chars):\n",
        "                    predicted_token = candidate\n",
        "                    break\n",
        "\n",
        "            replaced_words.append(predicted_token)  # ç½®æ›å¾Œã®å˜èªã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
        "        else:\n",
        "            replaced_words.append(word)  # ç¦æ­¢æ–‡å­—ã‚’å«ã¾ãªã„å ´åˆã¯å…ƒã®å˜èªã‚’ä¿æŒ\n",
        "\n",
        "    return \" \".join(replaced_words)\n",
        "\n",
        "# å…¥åŠ›æ–‡\n",
        "original_sentence = \"yesterday evening, i walked along the beach, listening to the sound of the waves and feeling the cool breeze on my face.\"\n",
        "# original_sentence = \"yesterday evening, i walked along the beach, listening to the sound of the waves and feeling the cool breeze on my face. the sunset painted the sky with shades of orange and pink, creating a breathtaking view. later, i met my family for dinner at a seafood restaurant, where we shared stories and enjoyed delicious dishes together. it was a peaceful and memorable evening.\"\n",
        "\n",
        "# ç¦æ­¢æ–‡å­—\n",
        "forbidden_chars = [\"a\"]\n",
        "\n",
        "# å¤‰æ›å®Ÿè¡Œ\n",
        "lipogrammed_sentence = lipogram_sentence_english(original_sentence, model, tokenizer, forbidden_chars)\n",
        "\n",
        "print(\"\\nOriginal sentence:\\n\", original_sentence)\n",
        "print(\"\\nLipogrammed sentence:\\n\", lipogrammed_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EJkEUZkCiqm"
      },
      "source": [
        "###è©•ä¾¡ã—ã¦ã¿ãŸ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LULw2h9SCltB",
        "outputId": "b761dc21-62eb-4216-d309-57e7638f39a5"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# æ¯”è¼ƒã™ã‚‹æ–‡ç« \n",
        "# sentence1 = sentence\n",
        "# sentence2 = new_sentence\n",
        "sentence1 = original_sentence\n",
        "sentence2 = lipogrammed_sentence\n",
        "\n",
        "# æ–‡ç« ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›\n",
        "embeddings1 = model.encode(sentence1, convert_to_tensor=True)\n",
        "embeddings2 = model.encode(sentence2, convert_to_tensor=True)\n",
        "\n",
        "# ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®è¨ˆç®—\n",
        "cosine_score = util.pytorch_cos_sim(embeddings1, embeddings2)[0][0]\n",
        "\n",
        "print(f\"æ–‡ç« 1ã¨æ–‡ç« 2ã®é¡ä¼¼åº¦: {cosine_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It5nSb3wm6Hi"
      },
      "source": [
        "#æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRsTnj_qzZyA",
        "outputId": "284209a1-801f-4c87-e41b-22407fc90bf1"
      },
      "outputs": [],
      "source": [
        "!apt install -y mecab libmecab-dev mecab-ipadic-utf8\n",
        "!pip uninstall -y fugashi unidic_lite\n",
        "!pip install fugashi[unidic-lite]\n",
        "\n",
        "!pip install fugashi\n",
        "!pip install ipadic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8Y2q0-Y70Gg"
      },
      "outputs": [],
      "source": [
        "openai.api_key = \"your-api-key\"  # ã”è‡ªèº«ã®APIã‚­ãƒ¼ã‚’ã“ã“ã«è¨­å®š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKf85h1ul_gm",
        "outputId": "602dcdb9-6177-4a75-9f08-61690ef58f9a"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from fugashi import Tagger\n",
        "import random\n",
        "\n",
        "MODEL_NAME = \"gpt-4\"\n",
        "\n",
        "banned_chars = [\"ã•\", \"ã„\"]  # ç¦æ­¢æ–‡å­—\n",
        "\n",
        "tagger = Tagger()\n",
        "\n",
        "def katakana_to_hiragana(katakana):\n",
        "    return ''.join(chr(ord(c) - 0x60) if 'ã‚¡' <= c <= 'ãƒ³' else c for c in katakana)\n",
        "\n",
        "def get_reading(token):\n",
        "    try:\n",
        "        # ãƒˆãƒ¼ã‚¯ãƒ³ã®èª­ã¿ä»®åï¼ˆã‚«ãƒŠï¼‰ã‚’å–å¾—\n",
        "        reading = token.feature.kana\n",
        "        if reading != '*' and reading:\n",
        "            return katakana_to_hiragana(reading)\n",
        "    except Exception as e:\n",
        "        print(f\"ã‚¨ãƒ©ãƒ¼: {e}\")\n",
        "\n",
        "    # èª­ã¿ä»®åãŒå–å¾—ã§ããªã‹ã£ãŸå ´åˆã¯ã€å…ƒã®è¡¨å±¤å½¢ã‚’åŸºã«å¤‰æ›\n",
        "    return katakana_to_hiragana(token.surface)\n",
        "\n",
        "def get_pos(token):\n",
        "    try:\n",
        "        # å“è©ãŒãƒªã‚¹ãƒˆã®0ç•ªç›®ã«ã‚ã‚‹å ´åˆï¼ˆUnidicï¼‰\n",
        "        pos = token.feature.pos1\n",
        "        if pos != '*' and pos:\n",
        "            return pos\n",
        "    except Exception as e:\n",
        "        print(f\"ã‚¨ãƒ©ãƒ¼: {e}\")\n",
        "    return \"åè©\"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¨ã—ã¦åè©ã‚’è¿”ã™\n",
        "\n",
        "def contains_banned(text):\n",
        "    return any(c in text for c in banned_chars)\n",
        "\n",
        "def rewrite_token(original_word, context=None, previous_replacements=None, pos=None, max_attempts=5):\n",
        "    for attempt in range(max_attempts):\n",
        "        # OpenAI APIã§æ–°ã—ã„å˜èªå€™è£œã‚’ç”Ÿæˆ\n",
        "        prompt = f\"\"\"\n",
        "        ä»¥ä¸‹ã®å˜èªã€Œ{original_word}ã€ã¯ã€ç¦æ­¢æ–‡å­—ã€Œ{'ã€'.join(banned_chars)}ã€ã‚’å«ã‚€ãŸã‚ã€æ–‡è„ˆã«åˆã£ãŸè‡ªç„¶ãªè¡¨ç¾ã«**å˜èªå˜ä½**ã§è¨€ã„æ›ãˆã¦ãã ã•ã„ã€‚\n",
        "        ãƒ»æ–‡ã®æ–‡è„ˆï¼šã€Œ{context}ã€\n",
        "        ãƒ»å¯¾è±¡ã®å˜èªï¼šã€Œ{original_word}ã€\n",
        "        ãƒ»å“è©ï¼šã€Œ{pos}ã€\n",
        "        ãƒ»å‡ºåŠ›ã¯ç½®ãæ›ãˆãŸèªå¥ **ä¸€å˜èª** ã«ã—ã¦ãã ã•ã„ã€‚\n",
        "        ãƒ»å‡ºåŠ›ã™ã‚‹èªå¥ã¯å¿…ãš**{original_word}**ã¨é•ã†å˜èªã‚’å‡ºåŠ›ã—ã¦\n",
        "        ãƒ»ç¦æ­¢æ–‡å­—ã€Œ{'ã€'.join(banned_chars)}ã€ã‚’**è¡¨è¨˜ã«ã‚‚èª­ã¿ï¼ˆã²ã‚‰ãŒãªï¼‰ã«ã‚‚å«ã¾ãªã„ã“ã¨**ã€‚\n",
        "        \"\"\"\n",
        "\n",
        "        # OpenAI APIã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆæ–°ã—ã„ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼‰\n",
        "        response = openai.chat.completions.create(\n",
        "            model=MODEL_NAME,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.5,  # å°‘ã—ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’æŒãŸã›ã‚‹\n",
        "            max_tokens=100\n",
        "        )\n",
        "\n",
        "        # ç”Ÿæˆã•ã‚ŒãŸå€™è£œ\n",
        "        candidate = response.choices[0].message.content.strip()\n",
        "\n",
        "        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¦èª­ã¿ä»®åã‚’å–å¾—ã™ã‚‹\n",
        "        temp_token = list(tagger(candidate))[0]  # å¤‰æ›å¾Œã®å˜èªã‚’å†ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
        "        candidate_reading = get_reading(temp_token)\n",
        "\n",
        "        # ç¦æ­¢æ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ãªã„å ´åˆã«çµæœã‚’è¿”ã™\n",
        "        if not contains_banned(candidate) and not contains_banned(candidate_reading):\n",
        "            return candidate, candidate_reading\n",
        "\n",
        "    # å¤‰æ›å€™è£œãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€å…ƒã®å˜èªã‚’è¿”ã™\n",
        "    return original_word, get_reading(list(tagger(original_word))[0])\n",
        "\n",
        "def rewrite_sentence_with_reading_check(sentence):\n",
        "    tokens = list(tagger(sentence))\n",
        "    new_tokens = []\n",
        "    previous_replacements = set()  # å±¥æ­´ã‚’ä¿æŒ\n",
        "\n",
        "    for token in tokens:\n",
        "        surface = token.surface\n",
        "        reading = get_reading(token)\n",
        "        pos = get_pos(token)\n",
        "\n",
        "        print(token)\n",
        "\n",
        "        print(f\"ãƒˆãƒ¼ã‚¯ãƒ³ï¼š{surface}ï¼ˆèª­ã¿ï¼š{reading}ï¼‰\")\n",
        "\n",
        "        if contains_banned(surface) or contains_banned(reading):\n",
        "            print(f\"âŒ ç¦æ­¢æ–‡å­—ã‚’å«ã‚€ï¼š{surface}ï¼ˆèª­ã¿ï¼š{reading}ï¼‰\")\n",
        "            # ç½®ãæ›ãˆã‚’è¡Œã†\n",
        "            replacement, replacement_reading = rewrite_token(surface, sentence, previous_replacements=previous_replacements, pos=pos)\n",
        "\n",
        "            # ç½®ãæ›ãˆãŸå¾Œã«å†åº¦ç¦æ­¢æ–‡å­—ã«å¼•ã£ã‹ã‹ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
        "            while contains_banned(replacement) or contains_banned(replacement_reading):\n",
        "                replacement, replacement_reading = rewrite_token(replacement, sentence, previous_replacements=previous_replacements, pos=pos)\n",
        "\n",
        "            print(f\"ğŸ‘‰ ã€Œ{surface}ã€â†’ã€Œ{replacement}ã€ï¼ˆèª­ã¿ï¼š{replacement_reading}ï¼‰\")\n",
        "            new_tokens.append(replacement)\n",
        "        else:\n",
        "            new_tokens.append(surface)\n",
        "\n",
        "    return ''.join(new_tokens)\n",
        "\n",
        "# ğŸ”µ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ\n",
        "input_text = \"ã•ã‚‹ã‚‚æœ¨ã‹ã‚‰è½ã¡ã‚‹ã€‚çŠ¬ã‚‚æ­©ã‘ã°æ£’ã«å½“ãŸã‚‹\"\n",
        "print(\"ğŸ”µ å…ƒã®æ–‡ï¼š\", input_text)\n",
        "output_text = rewrite_sentence_with_reading_check(input_text)\n",
        "print(\"ğŸŸ¢ å¤‰æ›å¾Œï¼š\", output_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDMSyMcr7cwx",
        "outputId": "e3ceef41-5af4-41ca-9f1a-b996aeac00d4"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from fugashi import Tagger\n",
        "import random\n",
        "import re\n",
        "\n",
        "MODEL_NAME = \"gpt-4\"\n",
        "\n",
        "# ç¦æ­¢æ–‡å­—ï¼ˆè¡¨è¨˜ãƒ»èª­ã¿ä¸¡æ–¹ã‹ã‚‰æ’é™¤ï¼‰\n",
        "banned_chars = [\"ã•\", \"ã„\"]\n",
        "\n",
        "# å½¢æ…‹ç´ è§£æå™¨ï¼ˆUnidic æ¨å¥¨ï¼‰\n",
        "tagger = Tagger()\n",
        "\n",
        "# ã‚«ã‚¿ã‚«ãƒŠâ†’ã²ã‚‰ãŒãªå¤‰æ›\n",
        "def katakana_to_hiragana(katakana):\n",
        "    return ''.join(chr(ord(c) - 0x60) if 'ã‚¡' <= c <= 'ãƒ³' else c for c in katakana)\n",
        "\n",
        "# èª­ã¿ä»®åå–å¾—\n",
        "def get_reading(token):\n",
        "    try:\n",
        "        # Unidic ã®å ´åˆ feature.kana\n",
        "        reading = getattr(token.feature, \"kana\", None) or getattr(token.feature, \"reading\", None)\n",
        "        if reading and reading != \"*\":\n",
        "            return katakana_to_hiragana(reading)\n",
        "    except Exception as e:\n",
        "        print(f\"[èª­ã¿å–å¾—ã‚¨ãƒ©ãƒ¼] {token.surface} : {e}\")\n",
        "    return katakana_to_hiragana(token.surface)\n",
        "\n",
        "# å“è©å–å¾—ï¼ˆUnidic ã‚’å‰æã¨ã™ã‚‹ï¼‰\n",
        "def get_pos(token):\n",
        "    try:\n",
        "        pos = getattr(token.feature, \"pos1\", None)\n",
        "        return pos if pos and pos != \"*\" else \"åè©\"\n",
        "    except Exception as e:\n",
        "        print(f\"[å“è©å–å¾—ã‚¨ãƒ©ãƒ¼] {token.surface} : {e}\")\n",
        "    return \"åè©\"\n",
        "\n",
        "# ç¦æ­¢æ–‡å­—ã‚’å«ã‚€ã‹\n",
        "def contains_banned(text):\n",
        "    return any(c in text for c in banned_chars)\n",
        "\n",
        "# GPTã«ã‚ˆã‚‹è¨€ã„æ›ãˆç”Ÿæˆï¼ˆå˜èªå˜ä½ï¼‰\n",
        "def rewrite_token(original_word, context=None, previous_replacements=None, pos=None, max_attempts=5):\n",
        "    for attempt in range(max_attempts):\n",
        "        prompt = f\"\"\"\n",
        "ä»¥ä¸‹ã®å˜èªã€Œ{original_word}ã€ã¯ã€ç¦æ­¢æ–‡å­—ã€Œ{'ã€'.join(banned_chars)}ã€ã‚’å«ã‚€ãŸã‚ã€æ–‡è„ˆã«åˆã£ãŸè‡ªç„¶ãªè¡¨ç¾ã«**å˜èªå˜ä½**ã§è¨€ã„æ›ãˆã¦ãã ã•ã„ã€‚\n",
        "ãƒ»æ–‡ã®æ–‡è„ˆï¼šã€Œ{context}ã€\n",
        "ãƒ»å¯¾è±¡ã®å˜èªï¼šã€Œ{original_word}ã€\n",
        "ãƒ»å“è©ï¼šã€Œ{pos}ã€\n",
        "ãƒ»å‡ºåŠ›ã¯ç½®ãæ›ãˆãŸèªå¥ **ä¸€å˜èªã®ã¿** ã«ã—ã¦ãã ã•ã„ã€‚\n",
        "ãƒ»çµ¶å¯¾ã«èª¬æ˜æ–‡ã‚„è£œè¶³ã¯ä»˜ã‘ãšã€å˜èªã ã‘ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n",
        "ãƒ»å‡ºåŠ›ã™ã‚‹èªå¥ã¯å¿…ãš**{original_word}**ã¨ç•°ãªã‚‹æ–°ã—ã„å˜èªã§ã‚ã‚‹ã“ã¨ã€‚\n",
        "ãƒ»ç¦æ­¢æ–‡å­—ã€Œ{'ã€'.join(banned_chars)}ã€ã‚’**è¡¨è¨˜ã«ã‚‚èª­ã¿ï¼ˆã²ã‚‰ãŒãªï¼‰ã«ã‚‚å«ã¾ãªã„ã“ã¨**ã€‚\n",
        "\"\"\"\n",
        "\n",
        "        response = openai.chat.completions.create(\n",
        "            model=MODEL_NAME,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.5,\n",
        "            max_tokens=100\n",
        "        )\n",
        "\n",
        "        candidate = response.choices[0].message.content.strip()\n",
        "\n",
        "        # ä¸è¦ãªè¨˜å·ã‚’å‰Šé™¤ã—ã€ä¸€èªã ã‘æŠ½å‡º\n",
        "        candidate = re.sub(r'[ã€Œã€ã€ã€\"\\'ï¼ˆï¼‰()ï¼»ï¼½\\[\\]]', '', candidate).split()[0]\n",
        "\n",
        "        temp_token = list(tagger(candidate))[0]\n",
        "        candidate_reading = get_reading(temp_token)\n",
        "\n",
        "        if not contains_banned(candidate) and not contains_banned(candidate_reading):\n",
        "            return candidate, candidate_reading\n",
        "\n",
        "    # å¤±æ•—ã—ãŸå ´åˆã¯å…ƒã®å˜èªã‚’è¿”ã™\n",
        "    fallback_token = list(tagger(original_word))[0]\n",
        "    return original_word, get_reading(fallback_token)\n",
        "\n",
        "# æ–‡ã‚’å‡¦ç†ã—ã€ç¦æ­¢æ–‡å­—ã‚’å«ã‚€å˜èªã‚’å·®ã—æ›¿ãˆã‚‹\n",
        "def rewrite_sentence_with_reading_check(sentence):\n",
        "    tokens = list(tagger(sentence))\n",
        "    new_tokens = []\n",
        "    previous_replacements = set()\n",
        "\n",
        "    for token in tokens:\n",
        "        surface = token.surface\n",
        "        reading = get_reading(token)\n",
        "        pos = get_pos(token)\n",
        "\n",
        "        print(f\"ãƒˆãƒ¼ã‚¯ãƒ³ï¼š{surface}ï¼ˆèª­ã¿ï¼š{reading}ï¼‰\")\n",
        "\n",
        "        if contains_banned(surface) or contains_banned(reading):\n",
        "            print(f\"âŒ ç¦æ­¢æ–‡å­—ã‚’å«ã‚€ï¼š{surface}ï¼ˆèª­ã¿ï¼š{reading}ï¼‰\")\n",
        "            replacement, replacement_reading = rewrite_token(surface, sentence, previous_replacements, pos)\n",
        "\n",
        "            # å†ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€å¤§5å›ï¼‰\n",
        "            retry_count = 0\n",
        "            while (contains_banned(replacement) or contains_banned(replacement_reading)) and retry_count < 5:\n",
        "                replacement, replacement_reading = rewrite_token(replacement, sentence, previous_replacements, pos)\n",
        "                retry_count += 1\n",
        "\n",
        "            print(f\"ğŸ‘‰ ã€Œ{surface}ã€â†’ã€Œ{replacement}ã€ï¼ˆèª­ã¿ï¼š{replacement_reading}ï¼‰\")\n",
        "            new_tokens.append(replacement)\n",
        "        else:\n",
        "            new_tokens.append(surface)\n",
        "\n",
        "    return ''.join(new_tokens)\n",
        "\n",
        "# ğŸ”µ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ\n",
        "input_text = \"ã•ã‚‹ã‚‚æœ¨ã‹ã‚‰è½ã¡ã‚‹ã€‚çŠ¬ã‚‚æ­©ã‘ã°æ£’ã«å½“ãŸã‚‹\"\n",
        "print(\"ğŸ”µ å…ƒã®æ–‡ï¼š\", input_text)\n",
        "output_text = rewrite_sentence_with_reading_check(input_text)\n",
        "print(\"ğŸŸ¢ å¤‰æ›å¾Œï¼š\", output_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gorzxH151VBW",
        "outputId": "4f252446-2bc0-48eb-e741-4330e6ba495c"
      },
      "outputs": [],
      "source": [
        "from fugashi import Tagger\n",
        "\n",
        "def katakana_to_hiragana(katakana):\n",
        "    return ''.join([chr(ord(c) - 0x60) if 'ã‚¡' <= c <= 'ãƒ³' else c for c in katakana])\n",
        "\n",
        "def get_reading(token):\n",
        "    # Unidic ã§ã¯èª­ã¿ã¯ feature[7] ã«ã‚ã‚Šã¾ã™ï¼ˆãªã„å ´åˆã¯è¡¨å±¤å½¢ã‚’è¿”ã™ï¼‰\n",
        "    features = token.feature\n",
        "    if (features):\n",
        "        reading = features[6] #èª­ã¿ä»®åã‚’ä»£å…¥\n",
        "        print(features)\n",
        "        if reading != \"*\" and reading:\n",
        "            return katakana_to_hiragana(reading)\n",
        "    return token.surface\n",
        "\n",
        "# Unidic Lite ã‚’ä½¿ã†\n",
        "tagger = Tagger(\"-d /usr/local/lib/python3.11/dist-packages/unidic_lite/dicdir\")\n",
        "\n",
        "sentence = \"çŠ¬ã‚‚æ­©ã‘ã°æ£’ã«å½“ãŸã‚‹\"\n",
        "for token in tagger(sentence):\n",
        "    print(f\"è¡¨å±¤å½¢ï¼š{token.surface}ã€€èª­ã¿ï¼š{get_reading(token)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glYZbbQU-P6D",
        "outputId": "24a63ff4-49ea-47aa-a8f0-f45ecb1f134d"
      },
      "outputs": [],
      "source": [
        "!apt install -y mecab libmecab-dev mecab-ipadic-utf8\n",
        "!pip install mecab-python3 fugashi ipadic\n",
        "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git\n",
        "!echo yes | mecab-ipadic-neologd/bin/install.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4zT9qGzLRFe",
        "outputId": "f36222e4-7932-4731-c5a3-31d094ed6b89"
      },
      "outputs": [],
      "source": [
        "import fugashi\n",
        "import ipadic\n",
        "\n",
        "tagger = fugashi.GenericTagger(ipadic.MECAB_ARGS)\n",
        "text = \"ç§ã¯AIãŒå¥½ãã§ã™\"\n",
        "\n",
        "for word in tagger(text):\n",
        "    surface = word.surface\n",
        "    features = word.feature\n",
        "    print(f\"{surface}: {features}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "TPBXdYdi_HyB",
        "pyP0RRyd7raX",
        "_EJkEUZkCiqm"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

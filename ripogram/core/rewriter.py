"""
OpenAI GPT-4 based word rewriter for ripogram generation.
"""

import re
from typing import List, Set, Tuple, Optional
from openai import OpenAI
from .tokenizer import JapaneseTokenizer
from .utils import contains_banned


class RipogramRewriter:
    """Rewrite Japanese text to avoid banned characters using OpenAI GPT-4."""
    
    def __init__(self, api_key: str, model_name: str = "gpt-4"):
        """
        Initialize the rewriter.
        
        Args:
            api_key: OpenAI API key
            model_name: OpenAI model name (default: gpt-4)
        """
        self.client = OpenAI(api_key=api_key)
        self.model_name = model_name
        self.tokenizer = JapaneseTokenizer()
    
    def split_into_sentences(self, text: str) -> List[str]:
        """
        Split text into sentences based on Japanese punctuation.
        
        Args:
            text: Input text to split
            
        Returns:
            List of sentences
        """
        # Split by Japanese sentence endings (ã€‚ï¼ï¼Ÿ)
        sentences = re.split(r'([ã€‚ï¼ï¼Ÿ])', text)
        
        # Recombine sentences with their punctuation
        result = []
        for i in range(0, len(sentences) - 1, 2):
            if i + 1 < len(sentences):
                sentence = sentences[i] + sentences[i + 1]
                if sentence.strip():
                    result.append(sentence)
        
        # Handle case where text doesn't end with punctuation
        if len(sentences) % 2 == 1 and sentences[-1].strip():
            result.append(sentences[-1])
        
        return result
    
    def rewrite_token_with_context(
        self, 
        original_word: str, 
        current_sentence: str,
        full_text: str,
        banned_chars: List[str],
        failed_attempts: Optional[List[str]] = None,
        pos: str = "åè©",
        max_attempts: int = 10
    ) -> Tuple[str, str]:
        """
        Rewrite a single token using enhanced context information.
        
        Args:
            original_word: Original word to replace
            current_sentence: The sentence containing the word
            full_text: The complete text for broader context
            banned_chars: List of banned characters
            failed_attempts: List of previously failed replacement attempts
            pos: Part of speech
            max_attempts: Maximum retry attempts
            
        Returns:
            Tuple of (replacement_word, replacement_reading)
        """
        if failed_attempts is None:
            failed_attempts = []
        
        for attempt in range(max_attempts):
            # Build enhanced prompt with full context
            base_prompt = f"""
ä»¥ä¸‹ã®å˜èªã€Œ{original_word}ã€ã¯ã€ç¦æ­¢æ–‡å­—ã€Œ{'ã€'.join(banned_chars)}ã€ã‚’å«ã‚€ãŸã‚ã€æ–‡è„ˆã«åˆã£ãŸè‡ªç„¶ãªè¡¨ç¾ã«**å˜èªå˜ä½**ã§è¨€ã„æ›ãˆã¦ãã ã•ã„ã€‚

ã€æ—¥æœ¬èªãƒªãƒã‚°ãƒ©ãƒ ã®ãƒ«ãƒ¼ãƒ«ã€‘
ãƒ»ç¦æ­¢æ–‡å­—ã€Œ{'ã€'.join(banned_chars)}ã€ã¯**èª­ã¿ï¼ˆã²ã‚‰ãŒãªï¼‰**ã§ã®ä½¿ç”¨ãŒç¦æ­¢ã•ã‚Œã¦ã„ã¾ã™
ãƒ»å¤‰æ›å¾Œã®å˜èªã‚’**ã²ã‚‰ãŒãªã§èª­ã‚“ã æ™‚**ã«ç¦æ­¢æ–‡å­—ãŒä¸€æ–‡å­—ã§ã‚‚å«ã¾ã‚Œã¦ã¯ã„ã‘ã¾ã›ã‚“
ãƒ»ä¾‹ï¼šã€ŒçŒ«ã€ï¼ˆã­ã“ï¼‰ãŒç¦æ­¢ãªã‚‰ã€Œãƒã‚³ç§‘ã€ï¼ˆã­ã“ã‹ï¼‰ã‚‚ã€Œã­ã€ã€Œã“ã€ã‚’å«ã‚€ãŸã‚ç¦æ­¢ã§ã™

ã€æ–‡è„ˆæƒ…å ±ã€‘
ãƒ»å…¨ä½“ã®æ–‡ç« ï¼šã€Œ{full_text}ã€
ãƒ»ç¾åœ¨ã®æ–‡ï¼šã€Œ{current_sentence}ã€
ãƒ»å¯¾è±¡ã®å˜èªï¼šã€Œ{original_word}ã€
ãƒ»å“è©ï¼šã€Œ{pos}ã€

ã€é‡è¦ã€‘
ãƒ»æ–‡å…¨ä½“ã®æ„å‘³ã¨æµã‚Œã‚’ä¿æŒã—ã¦ãã ã•ã„
ãƒ»æ–‡æ³•çš„ã«è‡ªç„¶ãªè¡¨ç¾ã‚’é¸ã‚“ã§ãã ã•ã„
ãƒ»å¤‰æ›å¾Œã®å˜èªã®èª­ã¿ï¼ˆã²ã‚‰ãŒãªï¼‰ã«ç¦æ­¢æ–‡å­—ã€Œ{'ã€'.join(banned_chars)}ã€ãŒ**ä¸€æ–‡å­—ã‚‚å«ã¾ã‚Œãªã„**ã“ã¨
"""
            
            # Add failed attempts information if any
            if failed_attempts:
                base_prompt += f"""
ãƒ»ä»¥ä¸‹ã®å€™è£œã¯æ—¢ã«è©¦è¡Œæ¸ˆã¿ã§ä½¿ç”¨ã§ãã¾ã›ã‚“ï¼šã€Œ{'ã€ã€Œ'.join(failed_attempts)}ã€
ãƒ»ã“ã‚Œã‚‰ã¨ã¯**å…¨ãç•°ãªã‚‹**æ–°ã—ã„è¡¨ç¾ã‚’è€ƒãˆã¦ãã ã•ã„ã€‚
"""
            
            # Add strategy based on attempt number
            if attempt < 3:
                strategy = "ãƒ»æ–‡è„ˆã«æœ€ã‚‚é©ã—ãŸåŒç¾©èªã‚„é¡ç¾©èªã§è¨€ã„æ›ãˆã¦ãã ã•ã„ã€‚"
            elif attempt < 6:
                strategy = "ãƒ»ã‚ˆã‚Šåºƒã„æ¦‚å¿µã‚„ä¸Šä½æ¦‚å¿µã§ã€æ–‡ã®æµã‚Œã‚’ä¿ã¤è¡¨ç¾ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚"
            else:
                strategy = "ãƒ»æ–‡è„ˆã«å¿œã˜ãŸæ„è¨³ã‚„ã€æ–‡å…¨ä½“ã®æ„å‘³ã‚’ä¿ã¤åˆ¥ã®è¡¨ç¾æ–¹æ³•ã‚’è©¦ã—ã¦ãã ã•ã„ã€‚"
            
            prompt = base_prompt + strategy + """
ãƒ»å‡ºåŠ›ã¯ç½®ãæ›ãˆãŸèªå¥ **ä¸€å˜èªã®ã¿** ã«ã—ã¦ãã ã•ã„ã€‚
ãƒ»çµ¶å¯¾ã«èª¬æ˜æ–‡ã‚„è£œè¶³ã¯ä»˜ã‘ãšã€å˜èªã ã‘ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""
            
            try:
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.5,
                    max_tokens=100
                )
                
                candidate = response.choices[0].message.content
                if candidate is None:
                    continue
                candidate = candidate.strip()
                
                # Clean up the candidate (remove quotes, parentheses, etc.)
                candidate = re.sub(
                    r'[ã€Œã€ã€ã€"\'ï¼ˆï¼‰()ï¼»ï¼½\[\]]', '', candidate
                ).split()[0]
                
                # Tokenize the candidate to get its reading
                candidate_tokens = self.tokenizer.tokenize(candidate)
                if candidate_tokens:
                    # Get the full reading by concatenating all token readings
                    candidate_reading = ''.join([token['reading'] for token in candidate_tokens])
                    
                    # Check if candidate is valid (no banned characters)
                    if (not contains_banned(candidate, banned_chars) and
                            not contains_banned(candidate_reading, banned_chars)):
                        return candidate, candidate_reading
                    else:
                        # Add failed candidate to history for next attempt
                        if candidate not in failed_attempts:
                            failed_attempts.append(candidate)
                        
            except Exception as e:
                print(f"[API Error] Attempt {attempt + 1}: {e}")
                continue
        
        # Fallback: return original word if all attempts fail
        original_tokens = self.tokenizer.tokenize(original_word)
        original_reading = (original_tokens[0]['reading']
                            if original_tokens else original_word)
        return original_word, original_reading
    
    def rewrite_text_with_context(
        self, 
        text: str, 
        banned_chars: List[str],
        verbose: bool = False
    ) -> str:
        """
        Rewrite text using sentence-level context for improved accuracy.
        
        Args:
            text: Input text to rewrite
            banned_chars: List of banned characters
            verbose: Whether to print detailed output
            
        Returns:
            Rewritten text
        """
        sentences = self.split_into_sentences(text)
        rewritten_sentences = []
        
        for i, sentence in enumerate(sentences):
            if verbose:
                print(f"\nğŸ“ æ–‡ {i+1}: {sentence}")
                print("-" * 50)
            
            tokens = self.tokenizer.tokenize(sentence)
            new_tokens = []
            
            for token_info in tokens:
                surface = token_info['surface']
                reading = token_info['reading']
                pos = token_info['pos']
                
                if verbose:
                    print(f"ãƒˆãƒ¼ã‚¯ãƒ³ï¼š{surface}ï¼ˆèª­ã¿ï¼š{reading}ï¼‰")
                
                # Check if token contains banned characters
                if (contains_banned(surface, banned_chars) or
                        contains_banned(reading, banned_chars)):
                    
                    if verbose:
                        print(f"âŒ ç¦æ­¢æ–‡å­—ã‚’å«ã‚€ï¼š{surface}ï¼ˆèª­ã¿ï¼š{reading}ï¼‰")
                    
                    # Track failed attempts for this token
                    failed_attempts: List[str] = []
                    replacement = surface
                    replacement_reading = reading
                    
                    # Try to rewrite the token with enhanced context
                    max_retries = 5
                    for retry in range(max_retries):
                        if verbose and retry > 0:
                            print(f"   ğŸ”„ å†è©¦è¡Œ {retry}: å¤±æ•—å±¥æ­´ {failed_attempts}")
                        
                        replacement, replacement_reading = self.rewrite_token_with_context(
                            surface, sentence, text, banned_chars, failed_attempts.copy(), pos
                        )
                        
                        # Check if replacement is valid
                        surface_has_banned = contains_banned(replacement, banned_chars)
                        reading_has_banned = contains_banned(replacement_reading, banned_chars)
                        
                        if not surface_has_banned and not reading_has_banned:
                            # Success! Break out of retry loop
                            break
                        else:
                            # Always add failed attempt to history (avoid duplicates)
                            if replacement not in failed_attempts:
                                failed_attempts.append(replacement)
                            if verbose:
                                print(f"   âŒ å¤±æ•—: ã€Œ{replacement}ã€ã‚’å±¥æ­´ã«è¿½åŠ ")
                    
                    if verbose:
                        # Check if replacement still contains banned characters
                        surface_has_banned = contains_banned(replacement, banned_chars)
                        reading_has_banned = contains_banned(replacement_reading, banned_chars)
                        
                        if surface_has_banned or reading_has_banned:
                            print(f"âš ï¸  ã€Œ{surface}ã€â†’ã€Œ{replacement}ã€ï¼ˆèª­ã¿ï¼š{replacement_reading}ï¼‰")
                            if surface_has_banned:
                                print(f"   è¡¨è¨˜ã«ç¦æ­¢æ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã¾ã™: {replacement}")
                            if reading_has_banned:
                                # Show detailed reading analysis
                                banned_in_reading = [char for char in banned_chars if char in replacement_reading]
                                print(f"   èª­ã¿ã«ç¦æ­¢æ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã¾ã™: {replacement_reading}")
                                print(f"   æ¤œå‡ºã•ã‚ŒãŸç¦æ­¢æ–‡å­—: {banned_in_reading}")
                        else:
                            print(f"ğŸ‘‰ ã€Œ{surface}ã€â†’ã€Œ{replacement}ã€ï¼ˆèª­ã¿ï¼š{replacement_reading}ï¼‰")
                    
                    new_tokens.append(replacement)
                else:
                    new_tokens.append(surface)
            
            rewritten_sentence = ''.join(new_tokens)
            rewritten_sentences.append(rewritten_sentence)
            
            if verbose:
                print(f"ğŸŸ¢ å¤‰æ›å¾Œã®æ–‡: {rewritten_sentence}")
        
        return ''.join(rewritten_sentences)
    
    def rewrite_token(
        self, 
        original_word: str, 
        context: str, 
        banned_chars: List[str],
        failed_attempts: Optional[List[str]] = None,
        pos: str = "åè©",
        max_attempts: int = 10
    ) -> Tuple[str, str]:
        """
        Rewrite a single token using OpenAI GPT-4 with failure history.
        
        Args:
            original_word: Original word to replace
            context: Sentence context
            banned_chars: List of banned characters
            failed_attempts: List of previously failed replacement attempts
            pos: Part of speech
            max_attempts: Maximum retry attempts
            
        Returns:
            Tuple of (replacement_word, replacement_reading)
        """
        if failed_attempts is None:
            failed_attempts = []
        
        for attempt in range(max_attempts):
            # Build dynamic prompt based on attempt number and failed history
            base_prompt = f"""
ä»¥ä¸‹ã®å˜èªã€Œ{original_word}ã€ã¯ã€ç¦æ­¢æ–‡å­—ã€Œ{'ã€'.join(banned_chars)}ã€ã‚’å«ã‚€ãŸã‚ã€æ–‡è„ˆã«åˆã£ãŸè‡ªç„¶ãªè¡¨ç¾ã«**å˜èªå˜ä½**ã§è¨€ã„æ›ãˆã¦ãã ã•ã„ã€‚

ã€æ—¥æœ¬èªãƒªãƒã‚°ãƒ©ãƒ ã®ãƒ«ãƒ¼ãƒ«ã€‘
ãƒ»ç¦æ­¢æ–‡å­—ã€Œ{'ã€'.join(banned_chars)}ã€ã¯**èª­ã¿ï¼ˆã²ã‚‰ãŒãªï¼‰**ã§ã®ä½¿ç”¨ãŒç¦æ­¢ã•ã‚Œã¦ã„ã¾ã™
ãƒ»å¤‰æ›å¾Œã®å˜èªã‚’**ã²ã‚‰ãŒãªã§èª­ã‚“ã æ™‚**ã«ç¦æ­¢æ–‡å­—ãŒä¸€æ–‡å­—ã§ã‚‚å«ã¾ã‚Œã¦ã¯ã„ã‘ã¾ã›ã‚“
ãƒ»ä¾‹ï¼šã€ŒçŒ«ã€ï¼ˆã­ã“ï¼‰ãŒç¦æ­¢ãªã‚‰ã€Œãƒã‚³ç§‘ã€ï¼ˆã­ã“ã‹ï¼‰ã‚‚ã€Œã­ã€ã€Œã“ã€ã‚’å«ã‚€ãŸã‚ç¦æ­¢ã§ã™

ãƒ»æ–‡ã®æ–‡è„ˆï¼šã€Œ{context}ã€
ãƒ»å¯¾è±¡ã®å˜èªï¼šã€Œ{original_word}ã€
ãƒ»å“è©ï¼šã€Œ{pos}ã€
ãƒ»å¤‰æ›å¾Œã®å˜èªã®èª­ã¿ï¼ˆã²ã‚‰ãŒãªï¼‰ã«ç¦æ­¢æ–‡å­—ã€Œ{'ã€'.join(banned_chars)}ã€ãŒ**ä¸€æ–‡å­—ã‚‚å«ã¾ã‚Œãªã„**ã“ã¨
"""
            
            # Add failed attempts information if any
            if failed_attempts:
                base_prompt += f"""
ãƒ»ä»¥ä¸‹ã®å€™è£œã¯æ—¢ã«è©¦è¡Œæ¸ˆã¿ã§ä½¿ç”¨ã§ãã¾ã›ã‚“ï¼šã€Œ{'ã€ã€Œ'.join(failed_attempts)}ã€
ãƒ»ã“ã‚Œã‚‰ã¨ã¯**å…¨ãç•°ãªã‚‹**æ–°ã—ã„è¡¨ç¾ã‚’è€ƒãˆã¦ãã ã•ã„ã€‚
"""
            
            # Add strategy based on attempt number
            if attempt < 3:
                strategy = "ãƒ»ç›´æ¥çš„ãªåŒç¾©èªã‚„é¡ç¾©èªã§è¨€ã„æ›ãˆã¦ãã ã•ã„ã€‚"
            elif attempt < 6:
                strategy = "ãƒ»ã‚ˆã‚Šåºƒã„æ¦‚å¿µã‚„ä¸Šä½æ¦‚å¿µã§è¨€ã„æ›ãˆã¦ãã ã•ã„ã€‚"
            else:
                strategy = "ãƒ»æ–‡è„ˆã«å¿œã˜ãŸæ„è¨³ã‚„åˆ¥ã®è¡¨ç¾æ–¹æ³•ã‚’è©¦ã—ã¦ãã ã•ã„ã€‚"
            
            prompt = base_prompt + strategy + """
ãƒ»å‡ºåŠ›ã¯ç½®ãæ›ãˆãŸèªå¥ **ä¸€å˜èªã®ã¿** ã«ã—ã¦ãã ã•ã„ã€‚
ãƒ»çµ¶å¯¾ã«èª¬æ˜æ–‡ã‚„è£œè¶³ã¯ä»˜ã‘ãšã€å˜èªã ã‘ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""
            
            try:
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.5,
                    max_tokens=100
                )
                
                candidate = response.choices[0].message.content
                if candidate is None:
                    continue
                candidate = candidate.strip()
                
                # Clean up the candidate (remove quotes, parentheses, etc.)
                candidate = re.sub(
                    r'[ã€Œã€ã€ã€"\'ï¼ˆï¼‰()ï¼»ï¼½\[\]]', '', candidate
                ).split()[0]
                
                # Tokenize the candidate to get its reading
                candidate_tokens = self.tokenizer.tokenize(candidate)
                if candidate_tokens:
                    # Get the full reading by concatenating all token readings
                    candidate_reading = ''.join([token['reading'] for token in candidate_tokens])
                    
                    # Check if candidate is valid (no banned characters)
                    if (not contains_banned(candidate, banned_chars) and
                            not contains_banned(candidate_reading, banned_chars)):
                        return candidate, candidate_reading
                    else:
                        # Add failed candidate to history for next attempt
                        if candidate not in failed_attempts:
                            failed_attempts.append(candidate)
                        
            except Exception as e:
                print(f"[API Error] Attempt {attempt + 1}: {e}")
                continue
        
        # Fallback: return original word if all attempts fail
        original_tokens = self.tokenizer.tokenize(original_word)
        original_reading = (original_tokens[0]['reading']
                            if original_tokens else original_word)
        return original_word, original_reading
    
    def rewrite_sentence(
        self, 
        sentence: str, 
        banned_chars: List[str],
        verbose: bool = False
    ) -> str:
        """
        Rewrite a sentence to avoid banned characters.
        
        Args:
            sentence: Input sentence
            banned_chars: List of banned characters
            verbose: Whether to print detailed output
            
        Returns:
            Rewritten sentence
        """
        tokens = self.tokenizer.tokenize(sentence)
        new_tokens = []
        previous_replacements: Set[str] = set()
        
        for token_info in tokens:
            surface = token_info['surface']
            reading = token_info['reading']
            pos = token_info['pos']
            
            if verbose:
                print(f"ãƒˆãƒ¼ã‚¯ãƒ³ï¼š{surface}ï¼ˆèª­ã¿ï¼š{reading}ï¼‰")
            
            # Check if token contains banned characters
            if (contains_banned(surface, banned_chars) or
                    contains_banned(reading, banned_chars)):
                
                if verbose:
                    print(f"âŒ ç¦æ­¢æ–‡å­—ã‚’å«ã‚€ï¼š{surface}"
                          f"ï¼ˆèª­ã¿ï¼š{reading}ï¼‰")
                
                # Track failed attempts for this token
                failed_attempts: List[str] = []
                replacement = surface
                replacement_reading = reading
                
                # Try to rewrite the token with failure history
                max_retries = 5
                for retry in range(max_retries):
                    if verbose and retry > 0:
                        print(f"   ğŸ”„ å†è©¦è¡Œ {retry}: å¤±æ•—å±¥æ­´ {failed_attempts}")
                    
                    replacement, replacement_reading = self.rewrite_token(
                        surface, sentence, banned_chars, failed_attempts.copy(), pos
                    )
                    
                    # Check if replacement is valid
                    surface_has_banned = contains_banned(replacement, banned_chars)
                    reading_has_banned = contains_banned(replacement_reading, banned_chars)
                    
                    if not surface_has_banned and not reading_has_banned:
                        # Success! Break out of retry loop
                        break
                    else:
                        # Always add failed attempt to history (avoid duplicates)
                        if replacement not in failed_attempts:
                            failed_attempts.append(replacement)
                        if verbose:
                            print(f"   âŒ å¤±æ•—: ã€Œ{replacement}ã€ã‚’å±¥æ­´ã«è¿½åŠ ")
                
                if verbose:
                    # Check if replacement still contains banned characters
                    surface_has_banned = contains_banned(replacement, banned_chars)
                    reading_has_banned = contains_banned(replacement_reading, banned_chars)
                    
                    if surface_has_banned or reading_has_banned:
                        print(f"âš ï¸  ã€Œ{surface}ã€â†’ã€Œ{replacement}ã€"
                              f"ï¼ˆèª­ã¿ï¼š{replacement_reading}ï¼‰")
                        if surface_has_banned:
                            print(f"   è¡¨è¨˜ã«ç¦æ­¢æ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã¾ã™: {replacement}")
                        if reading_has_banned:
                            # Show detailed reading analysis
                            banned_in_reading = [char for char in banned_chars if char in replacement_reading]
                            print(f"   èª­ã¿ã«ç¦æ­¢æ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã¾ã™: {replacement_reading}")
                            print(f"   æ¤œå‡ºã•ã‚ŒãŸç¦æ­¢æ–‡å­—: {banned_in_reading}")
                    else:
                        print(f"ğŸ‘‰ ã€Œ{surface}ã€â†’ã€Œ{replacement}ã€"
                              f"ï¼ˆèª­ã¿ï¼š{replacement_reading}ï¼‰")
                
                new_tokens.append(replacement)
                previous_replacements.add(replacement)
            else:
                new_tokens.append(surface)
        
        return ''.join(new_tokens)
